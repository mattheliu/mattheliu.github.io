<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href="//cdn.staticfile.net/Primer/21.0.7/primer.css" rel="stylesheet" />
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="![](cover.jpg)

# LMDeploy 的量化和部署

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [1 环境配置](#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
- [2 服务部署](#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2)
  - [2.1 模型转换](#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2)
    - [2.1.1 在线转换](#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2)
    - [2.1.2 离线转换](#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2)
  - [2.2  TurboMind 推理+命令行本地对话](#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D)
  - [2.3 TurboMind推理+API服务](#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [2.4 网页 Demo 演示](#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA)
    - [2.4.1 TurboMind 服务作为后端](#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
    - [2.4.2 TurboMind 推理作为后端](#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
  - [2.5 TurboMind 推理 + Python 代码集成](#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90)
  - [2.6 这么多，头秃，有没有最佳实践](#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
    - [2.6.1 方案实践](#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5)
    - [2.6.2 模型配置实践](#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5)
- [3 模型量化](#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96)
  - [3.1 KV Cache 量化](#31-kv-cache-%E9%87%8F%E5%8C%96)
    - [3.1.1 量化步骤](#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.1.2 量化效果](#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.2 W4A16 量化](#32-w4a16-%E9%87%8F%E5%8C%96)
    - [3.2.1 量化步骤](#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.2.2 量化效果](#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.3 最佳实践](#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
- [参考资料](#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)
- [附录1：TritonServer 作为推理引擎](#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E)
  - [TritonServer环境配置](#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
  - [TritonServer推理+API服务](#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [TritonServer 服务作为后端](#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


## 1 环境配置

首先我们可以使用 `vgpu-smi ` 查看显卡资源使用情况。">
<meta property="og:title" content="第一期书生浦语大模型实战营-LMDeploy 大模型量化部署实践">
<meta property="og:description" content="![](cover.jpg)

# LMDeploy 的量化和部署

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [1 环境配置](#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
- [2 服务部署](#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2)
  - [2.1 模型转换](#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2)
    - [2.1.1 在线转换](#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2)
    - [2.1.2 离线转换](#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2)
  - [2.2  TurboMind 推理+命令行本地对话](#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D)
  - [2.3 TurboMind推理+API服务](#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [2.4 网页 Demo 演示](#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA)
    - [2.4.1 TurboMind 服务作为后端](#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
    - [2.4.2 TurboMind 推理作为后端](#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)
  - [2.5 TurboMind 推理 + Python 代码集成](#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90)
  - [2.6 这么多，头秃，有没有最佳实践](#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
    - [2.6.1 方案实践](#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5)
    - [2.6.2 模型配置实践](#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5)
- [3 模型量化](#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96)
  - [3.1 KV Cache 量化](#31-kv-cache-%E9%87%8F%E5%8C%96)
    - [3.1.1 量化步骤](#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.1.2 量化效果](#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.2 W4A16 量化](#32-w4a16-%E9%87%8F%E5%8C%96)
    - [3.2.1 量化步骤](#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)
    - [3.2.2 量化效果](#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)
  - [3.3 最佳实践](#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)
- [参考资料](#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)
- [附录1：TritonServer 作为推理引擎](#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E)
  - [TritonServer环境配置](#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)
  - [TritonServer推理+API服务](#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)
  - [TritonServer 服务作为后端](#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


## 1 环境配置

首先我们可以使用 `vgpu-smi ` 查看显卡资源使用情况。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--LMDeploy%20-da-mo-xing-liang-hua-bu-shu-shi-jian.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>第一期书生浦语大模型实战营-LMDeploy 大模型量化部署实践</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />

</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">第一期书生浦语大模型实战营-LMDeploy 大模型量化部署实践</h1>
<div class="title-right">
    <a href="https://mattheliu.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/mattheliu/mattheliu.github.io/issues/5" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p><a target="_blank" rel="noopener noreferrer" href="cover.jpg"><img src="cover.jpg" alt="" style="max-width: 100%;"></a></p>
<h1>LMDeploy 的量化和部署</h1>


<ul>
<li><a href="#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">1 环境配置</a></li>
<li><a href="#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2">2 服务部署</a>
<ul>
<li><a href="#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2">2.1 模型转换</a>
<ul>
<li><a href="#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2">2.1.1 在线转换</a></li>
<li><a href="#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2">2.1.2 离线转换</a></li>
</ul>
</li>
<li><a href="#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D">2.2  TurboMind 推理+命令行本地对话</a></li>
<li><a href="#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">2.3 TurboMind推理+API服务</a></li>
<li><a href="#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA">2.4 网页 Demo 演示</a>
<ul>
<li><a href="#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">2.4.1 TurboMind 服务作为后端</a></li>
<li><a href="#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">2.4.2 TurboMind 推理作为后端</a></li>
</ul>
</li>
<li><a href="#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90">2.5 TurboMind 推理 + Python 代码集成</a></li>
<li><a href="#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">2.6 这么多，头秃，有没有最佳实践</a>
<ul>
<li><a href="#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5">2.6.1 方案实践</a></li>
<li><a href="#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5">2.6.2 模型配置实践</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96">3 模型量化</a>
<ul>
<li><a href="#31-kv-cache-%E9%87%8F%E5%8C%96">3.1 KV Cache 量化</a>
<ul>
<li><a href="#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4">3.1.1 量化步骤</a></li>
<li><a href="#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C">3.1.2 量化效果</a></li>
</ul>
</li>
<li><a href="#32-w4a16-%E9%87%8F%E5%8C%96">3.2 W4A16 量化</a>
<ul>
<li><a href="#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4">3.2.1 量化步骤</a></li>
<li><a href="#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C">3.2.2 量化效果</a></li>
</ul>
</li>
<li><a href="#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5">3.3 最佳实践</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
<li><a href="#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E">附录1：TritonServer 作为推理引擎</a>
<ul>
<li><a href="#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">TritonServer环境配置</a></li>
<li><a href="#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1">TritonServer推理+API服务</a></li>
<li><a href="#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF">TritonServer 服务作为后端</a></li>
</ul>
</li>
</ul>

<h2>1 环境配置</h2>
<p>首先我们可以使用 <code class="notranslate">vgpu-smi </code> 查看显卡资源使用情况。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/0.png"><img src="img/0.png" alt="" style="max-width: 100%;"></a></p>
<p>大家可以使用系统给的 vscode 进行后面的开发。分别点击下图「1」和「2」的位置，就会在下方显示终端。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/add0.png"><img src="img/add0.png" alt="" style="max-width: 100%;"></a></p>
<p>可以点击终端（TERMINAL）窗口右侧的「+」号创建新的终端窗口。大家可以新开一个窗口，执行下面的命令实时观察 GPU 资源的使用情况。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ watch vgpu-smi</pre></div>
<p>结果如下图所示，该窗口会实时检测 GPU 卡的使用情况。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/add1.png"><img src="img/add1.png" alt="" style="max-width: 100%;"></a></p>
<p>接下来我们切换到刚刚的终端（就是上图右边的那个「bash」，下面的「watch」就是监控的终端），创建部署和量化需要的环境。建议大家使用官方提供的环境，使用 conda 直接复制。</p>
<p>这里 <code class="notranslate">/share/conda_envs</code> 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ conda create -n CONDA_ENV_NAME --clone /share/conda_envs/internlm-base</pre></div>
<ul>
<li>如果clone操作过慢，可采用如下操作:</li>
</ul>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ /root/share/install_conda_env_internlm_base.sh lmdeploy</pre></div>
<p>我们取 <code class="notranslate">CONDA_ENV_NAME</code> 为 <code class="notranslate">lmdeploy</code>，复制完成后，可以在本地查看环境。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ conda env list</pre></div>
<p>结果如下所示。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> conda environments:</span>
<span class="pl-c"><span class="pl-c">#</span></span>
base                  <span class="pl-k">*</span>  /root/.conda
lmdeploy                 /root/.conda/envs/lmdeploy</pre></div>
<p>然后激活环境。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ conda activate lmdeploy</pre></div>
<p>注意，环境激活后，左边会显示当前（也就是 <code class="notranslate">lmdeploy</code>）的环境名称，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/1.png"><img src="img/1.png" alt="" style="max-width: 100%;"></a></p>
<p>可以进入Python检查一下 PyTorch 和 lmdeploy 的版本。由于 PyTorch 在官方提供的环境里，我们应该可以看到版本显示，而 lmdeploy 需要我们自己安装，此处应该会提示“没有这个包”，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/2.png"><img src="img/2.png" alt="" style="max-width: 100%;"></a></p>
<p>lmdeploy 没有安装，我们接下来手动安装一下，建议安装最新的稳定版。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">$ pip install <span class="pl-s"><span class="pl-pds">'</span>lmdeploy[all]==v0.1.0<span class="pl-pds">'</span></span></pre></div>
<p>由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 <code class="notranslate">[all]</code>。然后可以再检查一下 lmdeploy 包，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/add3.png"><img src="img/add3.png" alt="" style="max-width: 100%;"></a></p>
<p>如果遇到类似 <code class="notranslate"> ModuleNotFoundError: No module named 'packaging'</code> 这样的错误，大家可以手动先安装一下。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install packaging</pre></div>
<p><code class="notranslate">packaging</code> 表示缺少的包名。</p>
<p>基础环境到这里就配置好了。</p>
<ul>
<li>如果遇到<code class="notranslate">lmdeploy: command not found</code>,移步<a href="https://cguue83gpz.feishu.cn/docx/Noi7d5lllo6DMGxkuXwclxXMn5f#H2w9drpHiogeOHxhK7PcdJCmn8c" rel="nofollow">QA文档</a></li>
</ul>
<h2>2 服务部署</h2>
<p>这一部分主要涉及本地推理和部署。我们先看一张图。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/lmdeploy.drawio.png"><img src="img/lmdeploy.drawio.png" alt="" style="max-width: 100%;"></a></p>
<p>我们把从架构上把整个服务流程分成下面几个模块。</p>
<ul>
<li>模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。</li>
<li>Client。可以理解为前端，与用户交互的地方。</li>
<li>API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。</li>
</ul>
<p>值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。</p>
<p>接下来，我们看一下lmdeploy提供的部署功能。</p>
<h3>2.1 模型转换</h3>
<p>使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。</p>
<p>TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 <a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<h4>2.1.1 在线转换</h4>
<p>lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：</p>
<ul>
<li>在 huggingface.co 上面通过 lmdeploy 量化的模型，如 <a href="https://huggingface.co/lmdeploy/llama2-chat-70b-4bit" rel="nofollow">llama2-70b-4bit</a>, <a href="https://huggingface.co/internlm/internlm-chat-20b-4bit" rel="nofollow">internlm-chat-20b-4bit</a></li>
<li>huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat</li>
</ul>
<p>示例如下：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 需要能访问 Huggingface 的网络环境</span>
lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b
lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b</pre></div>
<p>上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。</p>
<p>我们也可以直接启动本地的 Huggingface 模型，如下所示。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b</pre></div>
<p>以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。</p>
<h4>2.1.2 离线转换</h4>
<p>离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind  的格式，如下所示。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 转换模型（FastTransformer格式） TurboMind</span>
lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b</pre></div>
<p>这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/</pre></div>
<p>执行完成后将会在当前目录生成一个 <code class="notranslate">workspace</code> 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。</p>
<p>目录如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/4.png"><img src="img/4.png" alt="" style="max-width: 100%;"></a></p>
<p><code class="notranslate">weights</code> 和 <code class="notranslate">tokenizer</code> 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 <code class="notranslate">weights</code> 的目录，就会发现参数是按层和模块拆开的，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/5.png"><img src="img/5.png" alt="" style="max-width: 100%;"></a></p>
<p>每一份参数第一个 0 表示“层”的索引，后面的那个0表示 Tensor 并行的索引，因为我们只有一张卡，所以被拆分成 1 份。如果有两张卡可以用来推理，则会生成0和1两份，也就是说，会把同一个参数拆成两份。比如 <code class="notranslate">layers.0.attention.w_qkv.0.weight</code> 会变成 <code class="notranslate">layers.0.attention.w_qkv.0.weight</code> 和 <code class="notranslate">layers.0.attention.w_qkv.1.weight</code>。执行 <code class="notranslate">lmdeploy convert</code> 命令时，可以通过 <code class="notranslate">--tp</code> 指定（tp 表示 tensor parallel），该参数默认值为1（也就是一张卡）。</p>
<p><strong>关于Tensor并行</strong></p>
<p>Tensor并行一般分为行并行或列并行，原理如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/6.png"><img src="img/6.png" alt="" style="max-width: 100%;"></a></p>
<p align="center">列并行</p><p>
</p><p><a target="_blank" rel="noopener noreferrer" href="img/7.png"><img src="img/7.png" alt="" style="max-width: 100%;"></a></p>
<p align="center">行并行</p><p>
</p><p>简单来说，就是把一个大的张量（参数）分到多张卡上，分别计算各部分的结果，然后再同步汇总。</p>
<h3>2.2  TurboMind 推理+命令行本地对话</h3>
<p>模型转换完成后，我们就具备了使用模型推理的条件，接下来就可以进行真正的模型推理环节。</p>
<p>我们先尝试本地对话（<code class="notranslate">Bash Local Chat</code>），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind。所以说，实际和前面的架构图是有区别的。</p>
<p>这里支持多种方式运行，比如Turbomind、PyTorch、DeepSpeed。但 PyTorch 和 DeepSpeed 调用的其实都是 Huggingface 的 Transformers 包，PyTorch表示原生的 Transformer 包，DeepSpeed 表示使用了 DeepSpeed 作为推理框架。Pytorch/DeepSpeed 目前功能都比较弱，不具备生产能力，不推荐使用。</p>
<p>执行命令如下。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> Turbomind + Bash Local Chat</span>
lmdeploy chat turbomind ./workspace</pre></div>
<p>启动后就可以和它进行对话了，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/8.png"><img src="img/8.png" alt="" style="max-width: 100%;"></a></p>
<p>输入后两次回车，退出时输入<code class="notranslate">exit</code> 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端。</p>
<h3>2.3 TurboMind推理+API服务</h3>
<p>在上面的部分我们尝试了直接用命令行启动 Client，接下来我们尝试如何运用 lmdepoy 进行服务化。</p>
<p>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。我们推荐使用 TurboMind，TritonServer 使用方式详见《附录1》。</p>
<p>首先，通过下面命令启动服务。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span>
lmdeploy serve api_server ./workspace \
	--server_name 0.0.0.0 \
	--server_port 23333 \
	--instance_num 64 \
	--tp 1</pre></div>
<p>上面的参数中 <code class="notranslate">server_name</code> 和 <code class="notranslate">server_port</code> 分别表示服务地址和端口，<code class="notranslate">tp</code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code class="notranslate">instance_num</code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/11.png"><img src="img/11.png" alt="" style="max-width: 100%;"></a></p>
<p>然后，我们可以新开一个窗口，执行下面的 Client 命令。如果使用官方机器，可以打开 vscode 的 Terminal，执行下面的命令。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> ChatApiClient+ApiServer（注意是http协议，需要加http）</span>
lmdeploy serve api_client http://localhost:23333</pre></div>
<p>如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/12.png"><img src="img/12.png" alt="" style="max-width: 100%;"></a></p>
<p>当然，刚刚我们启动的是 API Server，自然也有相应的接口。可以直接打开 <code class="notranslate">http://{host}:23333</code> 查看，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/13.png"><img src="img/13.png" alt="" style="max-width: 100%;"></a></p>
<blockquote>
<p>注意，这一步由于 Server 在远程服务器上，所以本地需要做一下 ssh 转发才能直接访问（与第一部分操作一样），命令如下：</p>
<p>ssh -CNg -L 23333:127.0.0.1:23333 <a href="mailto:root@ssh.intern-ai.org.cn">root@ssh.intern-ai.org.cn</a> -p &lt;你的ssh端口号&gt;</p>
<p>而执行本命令需要添加本机公钥，公钥添加后等待几分钟即可生效。ssh 端口号就是下面图片里的 33087。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/20.png"><img src="img/20.png" alt="" style="max-width: 100%;"></a></p>
</blockquote>
<p>这里一共提供了 4 个 HTTP 的接口，任何语言都可以对其进行调用，我们以 <code class="notranslate">v1/chat/completions</code> 接口为例，简单试一下。</p>
<p>接口请求参数如下：</p>
<div class="highlight highlight-source-json"><pre class="notranslate">{
  <span class="pl-ent">"model"</span>: <span class="pl-s"><span class="pl-pds">"</span>internlm-chat-7b<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"messages"</span>: <span class="pl-s"><span class="pl-pds">"</span>写一首春天的诗<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"temperature"</span>: <span class="pl-c1">0.7</span>,
  <span class="pl-ent">"top_p"</span>: <span class="pl-c1">1</span>,
  <span class="pl-ent">"n"</span>: <span class="pl-c1">1</span>,
  <span class="pl-ent">"max_tokens"</span>: <span class="pl-c1">512</span>,
  <span class="pl-ent">"stop"</span>: <span class="pl-c1">false</span>,
  <span class="pl-ent">"stream"</span>: <span class="pl-c1">false</span>,
  <span class="pl-ent">"presence_penalty"</span>: <span class="pl-c1">0</span>,
  <span class="pl-ent">"frequency_penalty"</span>: <span class="pl-c1">0</span>,
  <span class="pl-ent">"user"</span>: <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"repetition_penalty"</span>: <span class="pl-c1">1</span>,
  <span class="pl-ent">"renew_session"</span>: <span class="pl-c1">false</span>,
  <span class="pl-ent">"ignore_eos"</span>: <span class="pl-c1">false</span>
}</pre></div>
<p>请求结果如下。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/16.png"><img src="img/16.png" alt="" style="max-width: 100%;"></a></p>
<h3>2.4 网页 Demo 演示</h3>
<p>这一部分主要是将 Gradio 作为前端 Demo 演示。在上一节的基础上，我们不执行后面的 <code class="notranslate">api_client</code> 或 <code class="notranslate">triton_client</code>，而是执行 <code class="notranslate">gradio</code>。</p>
<blockquote>
<p>由于 Gradio 需要本地访问展示界面，因此也需要通过 ssh 将数据转发到本地。命令如下：</p>
<p>ssh -CNg -L 6006:127.0.0.1:6006 <a href="mailto:root@ssh.intern-ai.org.cn">root@ssh.intern-ai.org.cn</a> -p &lt;你的 ssh 端口号&gt;</p>
</blockquote>
<h4>2.4.1 TurboMind 服务作为后端</h4>
<p>API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span>
lmdeploy serve gradio http://0.0.0.0:23333 \
	--server_name 0.0.0.0 \
	--server_port 6006 \
	--restful_api True</pre></div>
<p>结果如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/17.png"><img src="img/17.png" alt="" style="max-width: 100%;"></a></p>
<h4>2.4.2 TurboMind 推理作为后端</h4>
<p>当然，Gradio 也可以直接和 TurboMind 连接，如下所示。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> Gradio+Turbomind(local)</span>
lmdeploy serve gradio ./workspace</pre></div>
<p>可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/19.png"><img src="img/19.png" alt="" style="max-width: 100%;"></a></p>
<h3>2.5 TurboMind 推理 + Python 代码集成</h3>
<p>前面介绍的都是通过 API 或某种前端与”模型推理/服务“进行交互，lmdeploy 还支持 Python 直接与 TurboMind 进行交互，如下所示。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">lmdeploy</span> <span class="pl-k">import</span> <span class="pl-s1">turbomind</span> <span class="pl-k">as</span> <span class="pl-s1">tm</span>

<span class="pl-c"># load model</span>
<span class="pl-s1">model_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/root/share/temp/model_repos/internlm-chat-7b/"</span>
<span class="pl-s1">tm_model</span> <span class="pl-c1">=</span> <span class="pl-s1">tm</span>.<span class="pl-v">TurboMind</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s1">model_path</span>, <span class="pl-s1">model_name</span><span class="pl-c1">=</span><span class="pl-s">'internlm-chat-20b'</span>)
<span class="pl-s1">generator</span> <span class="pl-c1">=</span> <span class="pl-s1">tm_model</span>.<span class="pl-en">create_instance</span>()

<span class="pl-c"># process query</span>
<span class="pl-s1">query</span> <span class="pl-c1">=</span> <span class="pl-s">"你好啊兄嘚"</span>
<span class="pl-s1">prompt</span> <span class="pl-c1">=</span> <span class="pl-s1">tm_model</span>.<span class="pl-s1">model</span>.<span class="pl-en">get_prompt</span>(<span class="pl-s1">query</span>)
<span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">tm_model</span>.<span class="pl-s1">tokenizer</span>.<span class="pl-en">encode</span>(<span class="pl-s1">prompt</span>)

<span class="pl-c"># inference</span>
<span class="pl-k">for</span> <span class="pl-s1">outputs</span> <span class="pl-c1">in</span> <span class="pl-s1">generator</span>.<span class="pl-en">stream_infer</span>(
        <span class="pl-s1">session_id</span><span class="pl-c1">=</span><span class="pl-c1">0</span>,
        <span class="pl-s1">input_ids</span><span class="pl-c1">=</span>[<span class="pl-s1">input_ids</span>]):
    <span class="pl-s1">res</span>, <span class="pl-s1">tokens</span> <span class="pl-c1">=</span> <span class="pl-s1">outputs</span>[<span class="pl-c1">0</span>]

<span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-s1">tm_model</span>.<span class="pl-s1">tokenizer</span>.<span class="pl-en">decode</span>(<span class="pl-s1">res</span>.<span class="pl-en">tolist</span>())
<span class="pl-en">print</span>(<span class="pl-s1">response</span>)</pre></div>
<p>在上面的代码中，我们首先加载模型，然后构造输入，最后执行推理。</p>
<p>加载模型可以显式指定模型路径，也可以直接指定 Huggingface 的 repo_id，还可以使用上面生成过的 <code class="notranslate">workspace</code>。这里的 <code class="notranslate">tm.TurboMind</code> 其实是对 C++ TurboMind 的封装。</p>
<p>构造输入这里主要是把用户的 query 构造成 InternLLM 支持的输入格式，比如上面的例子中， <code class="notranslate">query</code> 是“你好啊兄嘚”，构造好的 Prompt 如下所示。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s">"""</span>
<span class="pl-s">&lt;|System|&gt;:You are an AI assistant whose name is InternLM (书生·浦语).</span>
<span class="pl-s">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span>
<span class="pl-s">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span>
<span class="pl-s"></span>
<span class="pl-s">&lt;|User|&gt;:你好啊兄嘚</span>
<span class="pl-s">&lt;|Bot|&gt;:</span>
<span class="pl-s">"""</span></pre></div>
<p>Prompt 其实就是增加了 <code class="notranslate">&lt;|System|&gt;</code> 消息和 <code class="notranslate">&lt;|User|&gt;</code> 消息（即用户的 <code class="notranslate">query</code>），以及一个 <code class="notranslate">&lt;|Bot|&gt;</code> 的标记，表示接下来该模型输出响应了。最终输出的响应内容如下所示。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s">"你好啊，有什么我可以帮助你的吗？"</span></pre></div>
<h3>2.6 这么多，头秃，有没有最佳实践</h3>
<h4>2.6.1 方案实践</h4>
<p>必——须——有！</p>
<p>首先说 “模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好，相关的 Benchmark 对比如下。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/add4.png"><img src="img/add4.png" alt="" style="max-width: 100%;"></a></p>
<p>上面的性能对比包括两个场景：</p>
<ul>
<li>场景一（前4张图）：固定的输入、输出 token 数（分别1和2048），测试Token输出吞吐量（output token throughput）。</li>
<li>场景二（第5张图）：使用真实数据，测试吞吐量（request throughput）。</li>
</ul>
<p>场景一中，BatchSize=64时，TurboMind 的吞吐量超过 2000 token/s，整体比 DeepSpeed 提升约 5% - 15%；BatchSize=32时，比 Huggingface 的Transformers 提升约 3 倍；其他BatchSize时 TurboMind 也表现出优异的性能。</p>
<p>场景二中，对比了 TurboMind 和 vLLM 在真实数据上的吞吐量（request throughput）指标，TurboMind 的效率比 vLLM 高 30%。</p>
<p>大家不妨亲自使用本地对话（Local Chat）感受一下性能差别（2.2 节），也可以执行我们提供的 <code class="notranslate">infer_compare.py</code> 脚本，示例如下。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 执行 Huggingface 的 Transformer</span>
python infer_compare.py hf
<span class="pl-c"><span class="pl-c">#</span> 执行LMDeploy</span>
python infer_compare.py lmdeploy</pre></div>
<p>LMDeploy应该是Transformers的3-5倍左右。</p>
<p>后面的 API 服务和 Client 就得分场景了。</p>
<ul>
<li>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务（2.3）。</li>
<li>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示（2.4.2）。</li>
<li>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python（2.5）。</li>
<li>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</li>
<li>我的项目是 C++ 写的，为什么不能直接用 TurboMind 的 C++ 接口？！必须可以！大佬可以右上角叉掉这个窗口啦。</li>
</ul>
<h4>2.6.2 模型配置实践</h4>
<p>不知道大家还有没有印象，在离线转换（2.1.2）一节，我们查看了 <code class="notranslate">weights</code> 的目录，里面存放的是模型按层、按并行卡拆分的参数，不过还有一个文件 <code class="notranslate">config.ini</code> 并不是模型参数，它里面存的主要是模型相关的配置信息。下面是一个示例。</p>
<div class="highlight highlight-source-ini"><pre class="notranslate"><span class="pl-en">[llama]</span>
<span class="pl-k">model_name</span> = internlm-chat-7b
<span class="pl-k">tensor_para_size</span> = 1
<span class="pl-k">head_num</span> = 32
<span class="pl-k">kv_head_num</span> = 32
<span class="pl-k">vocab_size</span> = 103168
<span class="pl-k">num_layer</span> = 32
<span class="pl-k">inter_size</span> = 11008
<span class="pl-k">norm_eps</span> = 1e-06
<span class="pl-k">attn_bias</span> = 0
<span class="pl-k">start_id</span> = 1
<span class="pl-k">end_id</span> = 2
<span class="pl-k">session_len</span> = 2056
<span class="pl-k">weight_type</span> = fp16
<span class="pl-k">rotary_embedding</span> = 128
<span class="pl-k">rope_theta</span> = 10000.0
<span class="pl-k">size_per_head</span> = 128
<span class="pl-k">group_size</span> = 0
<span class="pl-k">max_batch_size</span> = 64
<span class="pl-k">max_context_token_num</span> = 1
<span class="pl-k">step_length</span> = 1
<span class="pl-k">cache_max_entry_count</span> = 0.5
<span class="pl-k">cache_block_seq_len</span> = 128
<span class="pl-k">cache_chunk_size</span> = 1
<span class="pl-k">use_context_fmha</span> = 1
<span class="pl-k">quant_policy</span> = 0
<span class="pl-k">max_position_embeddings</span> = 2048
<span class="pl-k">rope_scaling_factor</span> = 0.0
<span class="pl-k">use_logn_attn</span> = 0</pre></div>
<p>其中，模型属性相关的参数不可更改，主要包括下面这些。</p>
<div class="highlight highlight-source-ini"><pre class="notranslate"><span class="pl-k">model_name</span> = llama2
<span class="pl-k">head_num</span> = 32
<span class="pl-k">kv_head_num</span> = 32
<span class="pl-k">vocab_size</span> = 103168
<span class="pl-k">num_layer</span> = 32
<span class="pl-k">inter_size</span> = 11008
<span class="pl-k">norm_eps</span> = 1e-06
<span class="pl-k">attn_bias</span> = 0
<span class="pl-k">start_id</span> = 1
<span class="pl-k">end_id</span> = 2
<span class="pl-k">rotary_embedding</span> = 128
<span class="pl-k">rope_theta</span> = 10000.0
<span class="pl-k">size_per_head</span> = 128</pre></div>
<p>和数据类型相关的参数也不可更改，主要包括两个。</p>
<div class="highlight highlight-source-ini"><pre class="notranslate"><span class="pl-k">weight_type</span> = fp16
<span class="pl-k">group_size</span> = 0</pre></div>
<p><code class="notranslate">weight_type</code> 表示权重的数据类型。目前支持 fp16 和 int4。int4 表示 4bit 权重。当 <code class="notranslate">weight_type</code> 为 4bit 权重时，<code class="notranslate">group_size</code> 表示 <code class="notranslate">awq</code> 量化权重时使用的 group 大小。</p>
<p>剩余参数包括下面几个。</p>
<div class="highlight highlight-source-ini"><pre class="notranslate"><span class="pl-k">tensor_para_size</span> = 1
<span class="pl-k">session_len</span> = 2056
<span class="pl-k">max_batch_size</span> = 64
<span class="pl-k">max_context_token_num</span> = 1
<span class="pl-k">step_length</span> = 1
<span class="pl-k">cache_max_entry_count</span> = 0.5
<span class="pl-k">cache_block_seq_len</span> = 128
<span class="pl-k">cache_chunk_size</span> = 1
<span class="pl-k">use_context_fmha</span> = 1
<span class="pl-k">quant_policy</span> = 0
<span class="pl-k">max_position_embeddings</span> = 2048
<span class="pl-k">rope_scaling_factor</span> = 0.0
<span class="pl-k">use_logn_attn</span> = 0</pre></div>
<p>一般情况下，我们并不需要对这些参数进行修改，但有时候为了满足特定需要，可能需要调整其中一部分配置值。这里主要介绍三个可能需要调整的参数。</p>
<ul>
<li>KV int8 开关：
<ul>
<li>对应参数为 <code class="notranslate">quant_policy</code>，默认值为 0，表示不使用 KV Cache，如果需要开启，则将该参数设置为 4。</li>
<li>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</li>
<li>当显存不足，或序列比较长时，建议打开此开关。</li>
</ul>
</li>
<li>外推能力开关：
<ul>
<li>对应参数为 <code class="notranslate">rope_scaling_factor</code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code class="notranslate">use_logn_attn</code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</li>
<li>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</li>
<li>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</li>
</ul>
</li>
<li>批处理大小：
<ul>
<li>对应参数为 <code class="notranslate">max_batch_size</code>，默认为 64，也就是我们在 API Server 启动时的 <code class="notranslate">instance_num</code> 参数。</li>
<li>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</li>
<li>建议根据请求量和最大的上下文长度，按实际情况调整。</li>
</ul>
</li>
</ul>
<h2>3 模型量化</h2>
<p>本部分内容主要介绍如何对模型进行量化。主要包括 KV Cache 量化和模型参数量化。总的来说，量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。</p>
<p>正式介绍 LMDeploy 量化方案前，需要先介绍两个概念：</p>
<ul>
<li>计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。</li>
<li>访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。</li>
</ul>
<p>常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。</p>
<p>那么，如何优化 LLM 模型推理中的访存密集问题呢？ 我们可以使用 <strong>KV Cache 量化</strong>和 <strong>4bit Weight Only 量化（W4A16）</strong>。KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）。</p>
<h3>3.1 KV Cache 量化</h3>
<h4>3.1.1 量化步骤</h4>
<p>KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步：</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<ul>
<li>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code class="notranslate">(num_heads, head_dim)</code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</li>
<li>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code class="notranslate">(hidden_dim, )</code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</li>
</ul>
<p>第一步执行命令如下：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 计算 minmax</span>
lmdeploy lite calibrate \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --calib_dataset <span class="pl-s"><span class="pl-pds">"</span>c4<span class="pl-pds">"</span></span> \
  --calib_samples 128 \
  --calib_seqlen 2048 \
  --work_dir ./quant_output</pre></div>
<p>在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，数据集选择 C4，输入模型后就会得到上面的各种统计值。值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</p>
<blockquote>
<p>这一步由于默认需要从 Huggingface 下载数据集，国内经常不成功。所以我们导出了需要的数据，大家需要对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code class="notranslate">calib_dataloader.py</code> 到安装目录替换该文件：<code class="notranslate">cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code class="notranslate">cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code></li>
</ul>
</blockquote>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">zp = (min+max) / 2
scale = (max-min) / 255
quant: q = round( (f-zp) / scale)
dequant: f = q <span class="pl-k">*</span> scale + zp</pre></div>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<p>第二步的执行命令如下：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 通过 minmax 获取量化参数</span>
lmdeploy lite kv_qparams \
  --work_dir ./quant_output  \
  --turbomind_dir workspace/triton_models/weights/ \
  --kv_sym False \
  --num_tp 1</pre></div>
<p>在这个命令中，<code class="notranslate">num_tp</code> 的含义前面介绍过，表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 <code class="notranslate">workspace</code> 的参数目录中以便后续使用。<code class="notranslate">kv_sym</code> 为 <code class="notranslate">True</code> 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</p>
<p>第三步：修改配置。也就是修改 <code class="notranslate">weights/config.ini</code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code class="notranslate">quant_policy</code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code class="notranslate">use_context_fmha</code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
<h4>3.1.2 量化效果</h4>
<p>官方给出了 <a href="https://huggingface.co/internlm/internlm-chat-7b" rel="nofollow">internlm-chat-7b</a> 模型在 KV Cache 量化前后的显存对比情况，如下表所示。</p>
<table role="table">
<thead>
<tr>
<th>batch_size</th>
<th>fp16 memory(MiB)</th>
<th>int8 memory(MiB)</th>
<th>diff(MiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>22337</td>
<td>18241</td>
<td>-4096</td>
</tr>
<tr>
<td>16</td>
<td>30593</td>
<td>22369</td>
<td>-8224</td>
</tr>
<tr>
<td>32</td>
<td>47073</td>
<td>30625</td>
<td>-16448</td>
</tr>
<tr>
<td>48</td>
<td>63553</td>
<td>38881</td>
<td>-24672</td>
</tr>
</tbody>
</table>
<p>可以看出，KV Cache 可以节约大约 20% 的显存。</p>
<p>同时，还在 <a href="https://github.com/open-compass/opencompass">opencompass</a> 平台上测试了量化前后的精准度（Accuracy）对比情况，如下表所示。</p>
<table role="table">
<thead>
<tr>
<th>task</th>
<th>dataset</th>
<th>metric</th>
<th>int8</th>
<th>fp16</th>
<th>diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Language</td>
<td>winogrande</td>
<td>accuracy</td>
<td>60.77</td>
<td>61.48</td>
<td>-0.71</td>
</tr>
<tr>
<td>Knowledge</td>
<td>nq</td>
<td>score</td>
<td>2.69</td>
<td>2.60</td>
<td>+0.09</td>
</tr>
<tr>
<td>Reasoning</td>
<td>gsm8k</td>
<td>accuracy</td>
<td>33.28</td>
<td>34.72</td>
<td>-1.44</td>
</tr>
<tr>
<td>Reasoning</td>
<td>bbh</td>
<td>naive_average</td>
<td>20.12</td>
<td>20.51</td>
<td>-0.39</td>
</tr>
<tr>
<td>Understanding</td>
<td>openbookqa_fact</td>
<td>accuracy</td>
<td>82.40</td>
<td>82.20</td>
<td>+0.20</td>
</tr>
<tr>
<td>Understanding</td>
<td>eprstmt-dev</td>
<td>accuracy</td>
<td>90.62</td>
<td>88.75</td>
<td>+1.87</td>
</tr>
<tr>
<td>Safety</td>
<td>crows_pairs</td>
<td>accuracy</td>
<td>32.56</td>
<td>31.43</td>
<td>+1.13</td>
</tr>
</tbody>
</table>
<p>可以看出，精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能得原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
<p>总结一下，KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升。</p>
<h3>3.2 W4A16 量化</h3>
<h4>3.2.1 量化步骤</h4>
<p>W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<p>第一步：同 1.3.1，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul>
<li>缩放参数。主要是性能上的考虑（回顾 PPT）。</li>
<li>整体量化。</li>
</ul>
<p>第二步的执行命令如下：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 量化权重模型</span>
lmdeploy lite auto_awq \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --w_bits 4 \
  --w_group_size 128 \
  --work_dir ./quant_output </pre></div>
<p>命令中 <code class="notranslate">w_bits</code> 表示量化的位数，<code class="notranslate">w_group_size</code> 表示量化分组统计的尺寸，<code class="notranslate">work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code class="notranslate">torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p>最后一步：转换成 TurboMind 格式。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 转换模型的layout，存放在默认路径 ./workspace 下</span>
lmdeploy convert  internlm-chat-7b ./quant_output \
    --model-format awq \
    --group-size 128</pre></div>
<p>这个 <code class="notranslate">group-size</code> 就是上一步的那个 <code class="notranslate">w_group_size</code>。如果不想和之前的 <code class="notranslate">workspace</code> 重复，可以指定输出目录：<code class="notranslate">--dst_path</code>，比如：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">lmdeploy convert  internlm-chat-7b ./quant_output \
    --model-format awq \
    --group-size 128 \
    --dst_path ./workspace_quant</pre></div>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
<p>最后再补充一点，量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>
<h4>3.2.2 量化效果</h4>
<p>官方在 NVIDIA GeForce RTX 4090 上测试了 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 BatchSize = 1，prompt_tokens=1，completion_tokens=512，结果如下表所示。</p>
<table role="table">
<thead>
<tr>
<th>model</th>
<th>llm-awq</th>
<th>mlc-llm</th>
<th>turbomind</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>112.9</td>
<td>159.4</td>
<td>206.4</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>N/A</td>
<td>90.7</td>
<td>115.8</td>
</tr>
</tbody>
</table>
<p>可以看出，TurboMind 相比其他框架速度优势非常显著，比 mlc-llm 快了将近 30%。</p>
<p>另外，也测试了 TurboMind 在不同精度和上下文长度下的显存占用情况，如下表所示。</p>
<table role="table">
<thead>
<tr>
<th>model(context length)</th>
<th>16bit(2048)</th>
<th>4bit(2048)</th>
<th>16bit(4096)</th>
<th>4bit(4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>15.1</td>
<td>6.3</td>
<td>16.2</td>
<td>7.5</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>OOM</td>
<td>10.3</td>
<td>OOM</td>
<td>12.0</td>
</tr>
</tbody>
</table>
<p>可以看出，4bit 模型可以降低 50-60% 的显存占用，效果非常明显。</p>
<p>总而言之，W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势。</p>
<h3>3.3 最佳实践</h3>
<p>本节是针对《模型量化》部分的最佳实践。</p>
<p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》。</p>
<p>量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多。</p>
<p>所以我们的建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul>
<li>Step1：优先尝试正常（非量化）版本，评估效果。
<ul>
<li>如果效果不行，需要尝试更大参数模型或者微调。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step2：尝试正常版本+KV Cache 量化，评估效果。
<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step3：尝试量化版本，评估效果。
<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step4：尝试量化版本+ KV Cache 量化，评估效果。
<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，使用方案。</li>
</ul>
</li>
</ul>
<p>简单流程如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/quant.drawio.png"><img src="img/quant.drawio.png" alt="" style="max-width: 100%;"></a></p>
<p>另外需要补充说明的是，使用哪种量化版本、开启哪些功能，除了上述流程外，<strong>还需要考虑框架、显卡的支持情况</strong>，比如有些框架可能不支持 W4A16 的推理，那即便转换好了也用不了。</p>
<p>根据实践经验，一般情况下：</p>
<ul>
<li>精度越高，显存占用越多，推理效率越低，但一般效果较好。</li>
<li>Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。</li>
<li>端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。</li>
</ul>
<p>以上是针对项目开发情况，如果是自己尝试（玩儿）的话：</p>
<ul>
<li>如果资源足够（有GPU卡很重要），那就用非量化的正常版本。</li>
<li>如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。</li>
<li>如果生成文本长度很长，显存不够，就开启 KV Cache。</li>
</ul>
<p>建议大家根据实际情况灵活选择方案。</p>
<h2>参考资料</h2>
<ul>
<li><a href="https://github.com/InternLM/lmdeploy/">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/665725861" rel="nofollow">仅需一块 3090 显卡，高效部署 InternLM-20B 模型 - 知乎</a></li>
</ul>
<h2>附录1：TritonServer 作为推理引擎</h2>
<h3>TritonServer环境配置</h3>
<blockquote>
<p>注意：本部分内容仅支持物理机上执行，不支持虚拟主机。</p>
</blockquote>
<p>使用 Triton Server 需要安装一下 Docker 及其他依赖。</p>
<p>先装一些基本的依赖。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">apt-get update
apt-get install cmake sudo -y</pre></div>
<p>然后是 Docker 安装。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> Add Docker's official GPG key:</span>
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="pl-k">|</span> sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

<span class="pl-c"><span class="pl-c">#</span> Add the repository to Apt sources:</span>
<span class="pl-c1">echo</span> \
  <span class="pl-s"><span class="pl-pds">"</span>deb [arch=<span class="pl-s"><span class="pl-pds">$(</span>dpkg --print-architecture<span class="pl-pds">)</span></span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu <span class="pl-cce">\</span></span>
<span class="pl-s">  <span class="pl-s"><span class="pl-pds">$(</span>. /etc/os-release <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$VERSION_CODENAME</span><span class="pl-pds">"</span></span><span class="pl-pds">)</span></span> stable<span class="pl-pds">"</span></span> <span class="pl-k">|</span> \
  sudo tee /etc/apt/sources.list.d/docker.list <span class="pl-k">&gt;</span> /dev/null
sudo apt-get update

<span class="pl-c"><span class="pl-c">#</span> install</span>
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</pre></div>
<p>安装后我们跑一个 HelloWorld。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> helloworld</span>
sudo docker run hello-world</pre></div>
<p>可以看到类似下面的画面，表示运行成功。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/3.png"><img src="img/3.png" alt="" style="max-width: 100%;"></a></p>
<h3>TritonServer推理+API服务</h3>
<blockquote>
<p>注意：这部分需要 Docker 服务。</p>
</blockquote>
<p>这里我们把提供模型推理服务的引擎从 TurboMind 换成了 TritonServer，启动命令就一行。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> ApiServer+Triton</span>
bash workspace/service_docker_up.sh</pre></div>
<p>这里会启动一个 TritonServer 的容器，如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/14.png"><img src="img/14.png" alt="" style="max-width: 100%;"></a></p>
<p>可以再开一个窗口执行 Client 命令。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> ChatTritonClient + TritonServer（注意是gRPC协议，不要用http）</span>
lmdeploy serve triton_client  localhost:33337</pre></div>
<p>结果如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/15.png"><img src="img/15.png" alt="" style="max-width: 100%;"></a></p>
<h3>TritonServer 服务作为后端</h3>
<p>使用过程同 2.4.1 小节。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> Gradio+TritonServer（注意是gRPC协议，不要用http）</span>
lmdeploy serve gradio localhost:33337 \
	--server_name 0.0.0.0 \
	--server_port 6006</pre></div>
<p>结果如下图所示。</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/18.png"><img src="img/18.png" alt="" style="max-width: 100%;"></a></p>
<h2>作业</h2>
<p>提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。</p>
<p><strong>基础作业：</strong></p>
<ul>
<li>使用 LMDeploy 以本地对话、网页Gradio、API服务中的一种方式部署 InternLM-Chat-7B 模型，生成 300 字的小故事（需截图）<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/1068db20-90a7-4328-80ec-f60b97afdda0"><img src="https://github.com/mattheliu/gitblog/assets/102272920/1068db20-90a7-4328-80ec-f60b97afdda0" alt="6W}$0SGYG{FYQ7)J1XJRM5H" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/e4c3ce79-a68e-4c34-a20b-3582f569ba11"><img src="https://github.com/mattheliu/gitblog/assets/102272920/e4c3ce79-a68e-4c34-a20b-3582f569ba11" alt="1706170561445" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/f5dc453c-ec10-48da-8edb-95de9b0e4ceb"><img src="https://github.com/mattheliu/gitblog/assets/102272920/f5dc453c-ec10-48da-8edb-95de9b0e4ceb" alt="1706170951023" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/ecabe7b2-bbfc-457a-8ca7-9192fdeb3861"><img src="https://github.com/mattheliu/gitblog/assets/102272920/ecabe7b2-bbfc-457a-8ca7-9192fdeb3861" alt="FY8}_3Z 61JEVX4{JHKVRZT" style="max-width: 100%;"></a></li>
</ul>
<p><strong>进阶作业（可选做）</strong></p>
<ul>
<li>将第四节课训练自我认知小助手模型使用 LMDeploy 量化部署到 OpenXLab 平台。</li>
<li>对internlm-chat-7b模型进行量化，并同时使用KV Cache量化，使用量化后的模型完成API服务的部署，分别对比模型量化前后和 KV Cache 量化前后的显存大小（将 bs设置为 1 和 max len 设置为512）。</li>
<li>在自己的任务数据集上任取若干条进行Benchmark测试，测试方向包括：<br>
（1）TurboMind推理+Python代码集成<br>
（2）在（1）的基础上采用W4A16量化<br>
（3）在（1）的基础上开启KV Cache量化<br>
（4）在（2）的基础上开启KV Cache量化<br>
（5）使用Huggingface推理</li>
</ul>
<p>备注：<strong>由于进阶作业较难，完成基础作业之后就可以先提交作业了，在后续的大作业项目中使用这些技术将作为重要的加分点！</strong></p>
<p><strong>整体实训营项目：</strong></p>
<p>时间周期：即日起致课程结束</p>
<p>即日开始可以在班级群中随机组队完成一个大作业项目，一些可提供的选题如下：</p>
<ul>
<li>人情世故大模型：一个帮助用户撰写新年祝福文案的人情事故大模型</li>
<li>中小学数学大模型：一个拥有一定数学解题能力的大模型</li>
<li>心理大模型：一个治愈的心理大模型</li>
<li>工具调用类项目：结合 Lagent 构建数据集训练 InternLM 模型，支持对 MMYOLO 等工具的调用</li>
</ul>
<p>其他基于书生·浦语工具链的小项目都在范围内，欢迎大家充分发挥想象力。</p>
</div>
<div style="font-size:small;margin-top:8px;float:right;"></div>
<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>
</div>
    <div id="footer">Copyright © <span id="year"></span><a href="https://mattheliu.github.io"> Leonliuzx-Blog </a>
<p>
<span id="runday"></span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a>
</p>

<script>
if(""!=""){
    var now=new Date();
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("year").innerHTML=now.getFullYear();
    if(""!=""){document.getElementById("runday").innerHTML=" • "+"网站运行"+diffDay+"天"+" • ";}
    else{document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";}
}
</script>
</div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n\n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);

function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","mattheliu/mattheliu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
