<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href="//cdn.staticfile.net/Primer/21.0.7/primer.css" rel="stylesheet" />
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="# 大模型评测教程

随着人工智能技术的快速发展， 大规模预训练自然语言模型成为了研究热点和关注焦点。">
<meta property="og:title" content="第一期书生浦语大模型实战营-OpenCompass 大模型评测">
<meta property="og:description" content="# 大模型评测教程

随着人工智能技术的快速发展， 大规模预训练自然语言模型成为了研究热点和关注焦点。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--OpenCompass%20-da-mo-xing-ping-ce.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>第一期书生浦语大模型实战营-OpenCompass 大模型评测</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />

</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">第一期书生浦语大模型实战营-OpenCompass 大模型评测</h1>
<div class="title-right">
    <a href="https://mattheliu.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/mattheliu/mattheliu.github.io/issues/6" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h1>大模型评测教程</h1>
<p>随着人工智能技术的快速发展， 大规模预训练自然语言模型成为了研究热点和关注焦点。OpenAI于2018年提出了第一代GPT模型，开辟了自然语言模型生成式预训练的路线。沿着这条路线，随后又陆续发布了GPT-2和GPT-3模型。与此同时，谷歌也探索了不同的大规模预训练模型方案，例如如T5, Flan等。OpenAI在2022年11月发布ChatGPT，展示了强大的问答能力，逻辑推理能力和内容创作能力，将模型提升到了实用水平，改变人们对大模型能力的认知。在2023年4月，OpenAI发布了新升级的GPT-4模型，通过引入多模态能力，进一步拓展了大语言模型的能力边界，朝着通用人工智能更进一步。ChatGPT和GPT-4推出之后，微软凭借强大的产品化能力迅速将其集成进搜索引擎和Office办公套件中，形成了New Bing和 Office Copilot等产品。谷歌也迅速上线了基于自家大语言模型PaLM和PaLM-2的Bard，与OpenAI和微软展开正面竞争。国内的多家企业和研究机构也在开展大模型的技术研发，百度，阿里，华为，商汤，讯飞等都发布了各自的国产语言大模型，清华，复旦等高校也相继发布了GLM, MOSS等模型。</p>
<p>为了准确和公正地评估大模型的能力，国内外机构在大模型评测上开展了大量的尝试和探索。斯坦福大学提出了较为系统的评测框架HELM，从准确性，安全性，鲁棒性和公平性等维度开展模型评测。纽约大学联合谷歌和Meta提出了SuperGLUE评测集，从推理能力，常识理解，问答能力等方面入手，构建了包括8个子任务的大语言模型评测数据集。加州大学伯克利分校提出了MMLU测试集，构建了涵盖高中和大学的多项考试，来评估模型的知识能力和推理能力。谷歌也提出了包含数理科学，编程代码，阅读理解，逻辑推理等子任务的评测集Big-Bench，涵盖200多个子任务，对模型能力进行系统化的评估。在中文评测方面，国内的学术机构也提出了如CLUE,CUGE等评测数据集，从文本分类，阅读理解，逻辑推理等方面评测语言模型的中文能力。</p>
<p>随着大模型的蓬勃发展，如何全面系统地评估大模型的各项能力成为了亟待解决的问题。由于大语言模型和多模态模型的能力强大，应用场景广泛，目前学术界和工业界的评测方案往往只关注模型的部分能力维度，缺少系统化的能力维度框架与评测方案。OpenCompass提供设计一套全面、高效、可拓展的大模型评测方案，对模型能力、性能、安全性等进行全方位的评估。OpenCompass提供分布式自动化的评测系统，支持对(语言/多模态)大模型开展全面系统的能力评估。</p>
<h1>OpenCompass介绍</h1>
<h2>评测对象</h2>
<p>本算法库的主要评测对象为语言大模型与多模态大模型。我们以语言大模型为例介绍评测的具体模型类型。</p>
<ul>
<li>
<p><strong>基座模型</strong>：一般是经过海量的文本数据以自监督学习的方式进行训练获得的模型（如OpenAI的GPT-3，Meta的LLaMA），往往具有强大的文字续写能力。</p>
</li>
<li>
<p><strong>对话模型</strong>：一般是在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（如OpenAI的ChatGPT、上海人工智能实验室的书生·浦语），能理解人类指令，具有较强的对话能力。</p>
</li>
</ul>
<h2>工具架构</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/tonysy/opencompass/assets/7881589/374d9cec-2ebc-442a-ab11-191a7d5cf5e9"><img src="https://github.com/tonysy/opencompass/assets/7881589/374d9cec-2ebc-442a-ab11-191a7d5cf5e9" alt="framework-cn" style="max-width: 100%;"></a></p>
<ul>
<li>模型层：大模型评测所涉及的主要模型种类，OpenCompass以基座模型和对话模型作为重点评测对象。</li>
<li>能力层：OpenCompass从本方案从通用能力和特色能力两个方面来进行评测维度设计。在模型通用能力方面，从语言、知识、理解、推理、安全等多个能力维度进行评测。在特色能力方面，从长文本、代码、工具、知识增强等维度进行评测。</li>
<li>方法层：OpenCompass采用客观评测与主观评测两种评测方式。客观评测能便捷地评估模型在具有确定答案（如选择，填空，封闭式问答等）的任务上的能力，主观评测能评估用户对模型回复的真实满意度，OpenCompass采用基于模型辅助的主观评测和基于人类反馈的主观评测两种方式。</li>
<li>工具层：OpenCompass提供丰富的功能支持自动化地开展大语言模型的高效评测。包括分布式评测技术，提示词工程，对接评测数据库，评测榜单发布，评测报告生成等诸多功能。</li>
</ul>
<h2>能力维度</h2>
<h3>设计思路</h3>
<p>为准确、全面、系统化地评估大语言模型的能力，OpenCompass从通用人工智能的角度出发，结合学术界的前沿进展和工业界的最佳实践，提出一套面向实际应用的模型能力评价体系。OpenCompass能力维度体系涵盖通用能力和特色能力两大部分。</p>
<p>通用能力涵盖学科综合能力、知识能力、语言能力、理解能力、推理能力、安全能力，共计六大维度构造立体全面的模型能力评价体系。</p>
<p>特色能力</p>
<h2>评测方法</h2>
<p>OpenCompass采取客观评测与主观评测相结合的方法。针对具有确定性答案的能力维度和场景，通过构造丰富完善的评测集，对模型能力进行综合评价。针对体现模型能力的开放式或半开放式的问题、模型安全问题等，采用主客观相结合的评测方式。</p>
<h3>客观评测</h3>
<p>针对具有标准答案的客观问题，我们可以我们可以通过使用定量指标比较模型的输出与标准答案的差异，并根据结果衡量模型的性能。同时，由于大语言模型输出自由度较高，在评测阶段，我们需要对其输入和输出作一定的规范和设计，尽可能减少噪声输出在评测阶段的影响，才能对模型的能力有更加完整和客观的评价。</p>
<p>为了更好地激发出模型在题目测试领域的能力，并引导模型按照一定的模板输出答案，OpenCompass采用提示词工程 （prompt engineering）和语境学习（in-context learning）进行客观评测。</p>
<p>在客观评测的具体实践中，我们通常采用下列两种方式进行模型输出结果的评测：</p>
<ul>
<li>
<p><strong>判别式评测</strong>：该评测方式基于将问题与候选答案组合在一起，计算模型在所有组合上的困惑度（perplexity），并选择困惑度最小的答案作为模型的最终输出。例如，若模型在 <code class="notranslate">问题? 答案1</code> 上的困惑度为 0.1，在 <code class="notranslate">问题? 答案2</code> 上的困惑度为 0.2，最终我们会选择 <code class="notranslate">答案1</code> 作为模型的输出。</p>
</li>
<li>
<p><strong>生成式评测</strong>：该评测方式主要用于生成类任务，如语言翻译、程序生成、逻辑分析题等。具体实践时，使用问题作为模型的原始输入，并留白答案区域待模型进行后续补全。我们通常还需要对其输出进行后处理，以保证输出满足数据集的要求。</p>
</li>
</ul>
<h3>主观评测</h3>
<p>语言表达生动精彩，变化丰富，大量的场景和能力无法凭借客观指标进行评测。针对如模型安全和模型语言能力的评测，以人的主观感受为主的评测更能体现模型的真实能力，并更符合大模型的实际使用场景。</p>
<p>OpenCompass采取的主观评测方案是指借助受试者的主观判断对具有对话能力的大语言模型进行能力评测。在具体实践中，我们提前基于模型的能力维度构建主观测试问题集合，并将不同模型对于同一问题的不同回复展现给受试者，收集受试者基于主观感受的评分。由于主观测试成本高昂，本方案同时也采用使用性能优异的大语言模拟人类进行主观打分。在实际评测中，本文将采用真实人类专家的主观评测与基于模型打分的主观评测相结合的方式开展模型能力评估。</p>
<p>在具体开展主观评测时，OpenComapss采用<strong>单模型回复满意度统计</strong>和<strong>多模型满意度比较</strong>两种方式开展具体的评测工作。</p>
<h1>快速开始</h1>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/open-compass/opencompass/assets/22607038/d063cae0-3297-4fd2-921a-366e0a24890b"><img src="https://github.com/open-compass/opencompass/assets/22607038/d063cae0-3297-4fd2-921a-366e0a24890b" alt="image" style="max-width: 100%;"></a></p>
<h2>概览</h2>
<p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：<strong>配置</strong> -&gt; <strong>推理</strong> -&gt; <strong>评估</strong> -&gt; <strong>可视化</strong>。</p>
<p><strong>配置</strong>：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。</p>
<p><strong>推理与评估</strong>：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。<strong>推理</strong>阶段主要是让模型从数据集产生输出，而<strong>评估</strong>阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。</p>
<p><strong>可视化</strong>：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。</p>
<p>接下来，我们将展示 OpenCompass 的基础用法，展示书生浦语在 <a href="https://cevalbenchmark.com/index.html#home" rel="nofollow">C-Eval</a> 基准任务上的评估。它们的配置文件可以在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中找到。</p>
<h2>安装</h2>
<h3>面向GPU的环境安装</h3>
<div class="highlight highlight-source-shell"><pre class="notranslate">conda create --name opencompass --clone=/root/share/conda_envs/internlm-base
<span class="pl-c1">source</span> activate opencompass
git clone https://github.com/open-compass/opencompass
<span class="pl-c1">cd</span> opencompass
pip install -e <span class="pl-c1">.</span></pre></div>
<p>有部分第三方功能,如代码能力基准测试 Humaneval 以及 Llama格式的模型评测,可能需要额外步骤才能正常运行，如需评测，详细步骤请参考<a href="https://opencompass.readthedocs.io/zh_CN/latest/get_started/installation.html" rel="nofollow">安装指南</a>。</p>
<h3>数据准备</h3>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 解压评测数据集到 data/ 处</span>
cp /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/
unzip OpenCompassData-core-20231110.zip

<span class="pl-c"><span class="pl-c">#</span> 将会在opencompass下看到data文件夹</span></pre></div>
<h3>查看支持的数据集和模型</h3>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 列出所有跟 internlm 及 ceval 相关的配置</span>
python tools/list_configs.py internlm ceval</pre></div>
<p>将会看到</p>
<pre lang="text" class="notranslate"><code class="notranslate">+--------------------------+--------------------------------------------------------+
| Model                    | Config Path                                            |
|--------------------------+--------------------------------------------------------|
| hf_internlm_20b          | configs/models/hf_internlm/hf_internlm_20b.py          |
| hf_internlm_7b           | configs/models/hf_internlm/hf_internlm_7b.py           |
| hf_internlm_chat_20b     | configs/models/hf_internlm/hf_internlm_chat_20b.py     |
| hf_internlm_chat_7b      | configs/models/hf_internlm/hf_internlm_chat_7b.py      |
| hf_internlm_chat_7b_8k   | configs/models/hf_internlm/hf_internlm_chat_7b_8k.py   |
| hf_internlm_chat_7b_v1_1 | configs/models/hf_internlm/hf_internlm_chat_7b_v1_1.py |
| internlm_7b              | configs/models/internlm/internlm_7b.py                 |
| ms_internlm_chat_7b_8k   | configs/models/ms_internlm/ms_internlm_chat_7b_8k.py   |
+--------------------------+--------------------------------------------------------+
+----------------------------+------------------------------------------------------+
| Dataset                    | Config Path                                          |
|----------------------------+------------------------------------------------------|
| ceval_clean_ppl            | configs/datasets/ceval/ceval_clean_ppl.py            |
| ceval_gen                  | configs/datasets/ceval/ceval_gen.py                  |
| ceval_gen_2daf24           | configs/datasets/ceval/ceval_gen_2daf24.py           |
| ceval_gen_5f30c7           | configs/datasets/ceval/ceval_gen_5f30c7.py           |
| ceval_ppl                  | configs/datasets/ceval/ceval_ppl.py                  |
| ceval_ppl_578f8d           | configs/datasets/ceval/ceval_ppl_578f8d.py           |
| ceval_ppl_93e5ce           | configs/datasets/ceval/ceval_ppl_93e5ce.py           |
| ceval_zero_shot_gen_bd40ef | configs/datasets/ceval/ceval_zero_shot_gen_bd40ef.py |
+----------------------------+------------------------------------------------------+
</code></pre>
<h3>启动评测</h3>
<p>确保按照上述步骤正确安装 OpenCompass 并准备好数据集后，可以通过以下命令评测 InternLM-Chat-7B 模型在 C-Eval 数据集上的性能。由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 <code class="notranslate">--debug</code> 模式启动评估，并检查是否存在问题。在 <code class="notranslate">--debug</code> 模式下，任务将按顺序执行，并实时打印输出。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python run.py --datasets ceval_gen --hf-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-kwargs padding_side=<span class="pl-s"><span class="pl-pds">'</span>left<span class="pl-pds">'</span></span> truncation=<span class="pl-s"><span class="pl-pds">'</span>left<span class="pl-pds">'</span></span> trust_remote_code=True --model-kwargs trust_remote_code=True device_map=<span class="pl-s"><span class="pl-pds">'</span>auto<span class="pl-pds">'</span></span> --max-seq-len 2048 --max-out-len 16 --batch-size 4 --num-gpus 1 --debug</pre></div>
<p>命令解析</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">--datasets ceval_gen \
--hf-path /share/temp/model_repos/internlm-chat-7b/ <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> HuggingFace 模型路径</span>
--tokenizer-path /share/temp/model_repos/internlm-chat-7b/ <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> HuggingFace tokenizer 路径（如果与模型路径相同，可以省略）</span>
--tokenizer-kwargs padding_side=<span class="pl-s"><span class="pl-pds">'</span>left<span class="pl-pds">'</span></span> truncation=<span class="pl-s"><span class="pl-pds">'</span>left<span class="pl-pds">'</span></span> trust_remote_code=True <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> 构建 tokenizer 的参数</span>
--model-kwargs device_map=<span class="pl-s"><span class="pl-pds">'</span>auto<span class="pl-pds">'</span></span> trust_remote_code=True <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> 构建模型的参数</span>
--max-seq-len 2048 <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> 模型可以接受的最大序列长度</span>
--max-out-len 16 <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> 生成的最大 token 数</span>
--batch-size 2  <span class="pl-cce">\ </span> <span class="pl-c"><span class="pl-c">#</span> 批量大小</span>
--num-gpus 1  <span class="pl-c"><span class="pl-c">#</span> 运行模型所需的 GPU 数量</span>
--debug</pre></div>
<p>如果一切正常，您应该看到屏幕上显示 “Starting inference process”：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">[2024-01-12 18:23:55,076] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...</pre></div>
<p>评测完成后，将会看到：</p>
<pre class="notranslate"><code class="notranslate">
dataset                                         version    metric         mode      opencompass.models.huggingface.HuggingFace_model_repos_internlm-chat-7b
----------------------------------------------  ---------  -------------  ------  -------------------------------------------------------------------------
ceval-computer_network                          db9ce2     accuracy       gen                                                                         31.58
ceval-operating_system                          1c2571     accuracy       gen                                                                         36.84
ceval-computer_architecture                     a74dad     accuracy       gen                                                                         28.57
ceval-college_programming                       4ca32a     accuracy       gen                                                                         32.43
ceval-college_physics                           963fa8     accuracy       gen                                                                         26.32
ceval-college_chemistry                         e78857     accuracy       gen                                                                         16.67
ceval-advanced_mathematics                      ce03e2     accuracy       gen                                                                         21.05
ceval-probability_and_statistics                65e812     accuracy       gen                                                                         38.89
ceval-discrete_mathematics                      e894ae     accuracy       gen                                                                         18.75
ceval-electrical_engineer                       ae42b9     accuracy       gen                                                                         35.14
ceval-metrology_engineer                        ee34ea     accuracy       gen                                                                         50
ceval-high_school_mathematics                   1dc5bf     accuracy       gen                                                                         22.22
ceval-high_school_physics                       adf25f     accuracy       gen                                                                         31.58
ceval-high_school_chemistry                     2ed27f     accuracy       gen                                                                         15.79
ceval-high_school_biology                       8e2b9a     accuracy       gen                                                                         36.84
ceval-middle_school_mathematics                 bee8d5     accuracy       gen                                                                         26.32
ceval-middle_school_biology                     86817c     accuracy       gen                                                                         61.9
ceval-middle_school_physics                     8accf6     accuracy       gen                                                                         63.16
ceval-middle_school_chemistry                   167a15     accuracy       gen                                                                         60
ceval-veterinary_medicine                       b4e08d     accuracy       gen                                                                         47.83
ceval-college_economics                         f3f4e6     accuracy       gen                                                                         41.82
ceval-business_administration                   c1614e     accuracy       gen                                                                         33.33
ceval-marxism                                   cf874c     accuracy       gen                                                                         68.42
ceval-mao_zedong_thought                        51c7a4     accuracy       gen                                                                         70.83
ceval-education_science                         591fee     accuracy       gen                                                                         58.62
ceval-teacher_qualification                     4e4ced     accuracy       gen                                                                         70.45
ceval-high_school_politics                      5c0de2     accuracy       gen                                                                         26.32
ceval-high_school_geography                     865461     accuracy       gen                                                                         47.37
ceval-middle_school_politics                    5be3e7     accuracy       gen                                                                         52.38
ceval-middle_school_geography                   8a63be     accuracy       gen                                                                         58.33
ceval-modern_chinese_history                    fc01af     accuracy       gen                                                                         73.91
ceval-ideological_and_moral_cultivation         a2aa4a     accuracy       gen                                                                         63.16
ceval-logic                                     f5b022     accuracy       gen                                                                         31.82
ceval-law                                       a110a1     accuracy       gen                                                                         25
ceval-chinese_language_and_literature           0f8b68     accuracy       gen                                                                         30.43
ceval-art_studies                               2a1300     accuracy       gen                                                                         60.61
ceval-professional_tour_guide                   4e673e     accuracy       gen                                                                         62.07
ceval-legal_professional                        ce8787     accuracy       gen                                                                         39.13
ceval-high_school_chinese                       315705     accuracy       gen                                                                         63.16
ceval-high_school_history                       7eb30a     accuracy       gen                                                                         70
ceval-middle_school_history                     48ab4a     accuracy       gen                                                                         59.09
ceval-civil_servant                             87d061     accuracy       gen                                                                         53.19
ceval-sports_science                            70f27b     accuracy       gen                                                                         52.63
ceval-plant_protection                          8941f9     accuracy       gen                                                                         59.09
ceval-basic_medicine                            c409d6     accuracy       gen                                                                         47.37
ceval-clinical_medicine                         49e82d     accuracy       gen                                                                         40.91
ceval-urban_and_rural_planner                   95b885     accuracy       gen                                                                         45.65
ceval-accountant                                002837     accuracy       gen                                                                         26.53
ceval-fire_engineer                             bc23f5     accuracy       gen                                                                         22.58
ceval-environmental_impact_assessment_engineer  c64e2d     accuracy       gen                                                                         64.52
ceval-tax_accountant                            3a5e3c     accuracy       gen                                                                         34.69
ceval-physician                                 6e277d     accuracy       gen                                                                         40.82
ceval-stem                                      -          naive_average  gen                                                                         35.09
ceval-social-science                            -          naive_average  gen                                                                         52.79
ceval-humanities                                -          naive_average  gen                                                                         52.58
ceval-other                                     -          naive_average  gen                                                                         44.36
ceval-hard                                      -          naive_average  gen                                                                         23.91
ceval                                           -          naive_average  gen                                                                         44.16
</code></pre>
<p>有关 <code class="notranslate">run.py</code> 支持的所有与 HuggingFace 相关的参数，请阅读 <a href="https://opencompass.readthedocs.io/zh-cn/latest/user_guides/experimentation.html#id2" rel="nofollow">评测任务发起</a></p>
<p>除了通过命令行配置实验外，OpenCompass 还允许用户在配置文件中编写实验的完整配置，并通过 <code class="notranslate">run.py</code> 直接运行它。配置文件是以 Python 格式组织的，并且必须包括 <code class="notranslate">datasets</code> 和 <code class="notranslate">models</code> 字段。</p>
<p>示例测试配置在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中。此配置通过 <a href="../user_guides/config.md#%E7%BB%A7%E6%89%BF%E6%9C%BA%E5%88%B6">继承机制</a> 引入所需的数据集和模型配置，并以所需格式组合 <code class="notranslate">datasets</code> 和 <code class="notranslate">models</code> 字段。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">mmengine</span>.<span class="pl-s1">config</span> <span class="pl-k">import</span> <span class="pl-s1">read_base</span>

<span class="pl-k">with</span> <span class="pl-en">read_base</span>():
    <span class="pl-k">from</span> .<span class="pl-s1">datasets</span>.<span class="pl-s1">siqa</span>.<span class="pl-s1">siqa_gen</span> <span class="pl-k">import</span> <span class="pl-s1">siqa_datasets</span>
    <span class="pl-k">from</span> .<span class="pl-s1">datasets</span>.<span class="pl-s1">winograd</span>.<span class="pl-s1">winograd_ppl</span> <span class="pl-k">import</span> <span class="pl-s1">winograd_datasets</span>
    <span class="pl-k">from</span> .<span class="pl-s1">models</span>.<span class="pl-s1">opt</span>.<span class="pl-s1">hf_opt_125m</span> <span class="pl-k">import</span> <span class="pl-s1">opt125m</span>
    <span class="pl-k">from</span> .<span class="pl-s1">models</span>.<span class="pl-s1">opt</span>.<span class="pl-s1">hf_opt_350m</span> <span class="pl-k">import</span> <span class="pl-s1">opt350m</span>

<span class="pl-s1">datasets</span> <span class="pl-c1">=</span> [<span class="pl-c1">*</span><span class="pl-s1">siqa_datasets</span>, <span class="pl-c1">*</span><span class="pl-s1">winograd_datasets</span>]
<span class="pl-s1">models</span> <span class="pl-c1">=</span> [<span class="pl-s1">opt125m</span>, <span class="pl-s1">opt350m</span>]</pre></div>
<p>运行任务时，我们只需将配置文件的路径传递给 <code class="notranslate">run.py</code>：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python run.py configs/eval_demo.py</pre></div>
<p>OpenCompass 提供了一系列预定义的模型配置，位于 <code class="notranslate">configs/models</code> 下。以下是与 <a href="https://github.com/open-compass/opencompass/blob/main/configs/models/opt/hf_opt_350m.py">opt-350m</a>（<code class="notranslate">configs/models/opt/hf_opt_350m.py</code>）相关的配置片段：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># 使用 `HuggingFaceCausalLM` 评估由 HuggingFace 的 `AutoModelForCausalLM` 支持的模型</span>
<span class="pl-k">from</span> <span class="pl-s1">opencompass</span>.<span class="pl-s1">models</span> <span class="pl-k">import</span> <span class="pl-v">HuggingFaceCausalLM</span>

<span class="pl-c"># OPT-350M</span>
<span class="pl-s1">opt350m</span> <span class="pl-c1">=</span> <span class="pl-en">dict</span>(
       <span class="pl-s1">type</span><span class="pl-c1">=</span><span class="pl-v">HuggingFaceCausalLM</span>,
       <span class="pl-c"># `HuggingFaceCausalLM` 的初始化参数</span>
       <span class="pl-s1">path</span><span class="pl-c1">=</span><span class="pl-s">'facebook/opt-350m'</span>,
       <span class="pl-s1">tokenizer_path</span><span class="pl-c1">=</span><span class="pl-s">'facebook/opt-350m'</span>,
       <span class="pl-s1">tokenizer_kwargs</span><span class="pl-c1">=</span><span class="pl-en">dict</span>(
           <span class="pl-s1">padding_side</span><span class="pl-c1">=</span><span class="pl-s">'left'</span>,
           <span class="pl-s1">truncation_side</span><span class="pl-c1">=</span><span class="pl-s">'left'</span>,
           <span class="pl-s1">proxies</span><span class="pl-c1">=</span><span class="pl-c1">None</span>,
           <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>),
       <span class="pl-s1">model_kwargs</span><span class="pl-c1">=</span><span class="pl-en">dict</span>(<span class="pl-s1">device_map</span><span class="pl-c1">=</span><span class="pl-s">'auto'</span>),
       <span class="pl-c"># 下面是所有模型的共同参数，不特定于 HuggingFaceCausalLM</span>
       <span class="pl-s1">abbr</span><span class="pl-c1">=</span><span class="pl-s">'opt350m'</span>,               <span class="pl-c"># 结果显示的模型缩写</span>
       <span class="pl-s1">max_seq_len</span><span class="pl-c1">=</span><span class="pl-c1">2048</span>,             <span class="pl-c"># 整个序列的最大长度</span>
       <span class="pl-s1">max_out_len</span><span class="pl-c1">=</span><span class="pl-c1">100</span>,              <span class="pl-c"># 生成的最大 token 数</span>
       <span class="pl-s1">batch_size</span><span class="pl-c1">=</span><span class="pl-c1">64</span>,                <span class="pl-c"># 批量大小</span>
       <span class="pl-s1">run_cfg</span><span class="pl-c1">=</span><span class="pl-en">dict</span>(<span class="pl-s1">num_gpus</span><span class="pl-c1">=</span><span class="pl-c1">1</span>),     <span class="pl-c"># 该模型所需的 GPU 数量</span>
    )</pre></div>
<p>使用配置时，我们可以通过命令行参数 <code class="notranslate">--models</code> 指定相关文件，或使用继承机制将模型配置导入到配置文件中的 <code class="notranslate">models</code> 列表中。</p>
<p>与模型类似，数据集的配置文件也提供在 <code class="notranslate">configs/datasets</code> 下。用户可以在命令行中使用 <code class="notranslate">--datasets</code>，或通过继承在配置文件中导入相关配置</p>
<p>下面是来自 <code class="notranslate">configs/eval_demo.py</code> 的与数据集相关的配置片段：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">mmengine</span>.<span class="pl-s1">config</span> <span class="pl-k">import</span> <span class="pl-s1">read_base</span>  <span class="pl-c"># 使用 mmengine.read_base() 读取基本配置</span>

<span class="pl-k">with</span> <span class="pl-en">read_base</span>():
    <span class="pl-c"># 直接从预设的数据集配置中读取所需的数据集配置</span>
    <span class="pl-k">from</span> .<span class="pl-s1">datasets</span>.<span class="pl-s1">winograd</span>.<span class="pl-s1">winograd_ppl</span> <span class="pl-k">import</span> <span class="pl-s1">winograd_datasets</span>  <span class="pl-c"># 读取 Winograd 配置，基于 PPL（困惑度）进行评估</span>
    <span class="pl-k">from</span> .<span class="pl-s1">datasets</span>.<span class="pl-s1">siqa</span>.<span class="pl-s1">siqa_gen</span> <span class="pl-k">import</span> <span class="pl-s1">siqa_datasets</span>  <span class="pl-c"># 读取 SIQA 配置，基于生成进行评估</span>

<span class="pl-s1">datasets</span> <span class="pl-c1">=</span> [<span class="pl-c1">*</span><span class="pl-s1">siqa_datasets</span>, <span class="pl-c1">*</span><span class="pl-s1">winograd_datasets</span>]       <span class="pl-c"># 最终的配置需要包含所需的评估数据集列表 'datasets'</span></pre></div>
<p>数据集配置通常有两种类型：'ppl' 和 'gen'，分别指示使用的评估方法。其中 <code class="notranslate">ppl</code> 表示辨别性评估，<code class="notranslate">gen</code> 表示生成性评估。</p>
<p>此外，<a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections">configs/datasets/collections</a> 收录了各种数据集集合，方便进行综合评估。OpenCompass 通常使用 <a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections/base_medium.py"><code class="notranslate">base_medium.py</code></a> 进行全面的模型测试。要复制结果，只需导入该文件，例如：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python run.py --models hf_llama_7b --datasets base_medium</pre></div>
<p>OpenCompass 通常假定运行环境网络是可用的。如果您遇到网络问题或希望在离线环境中运行 OpenCompass，请参阅 <a href="https://opencompass.readthedocs.io/zh-cn/latest/get_started/faq.html" rel="nofollow">FAQ - 网络 - Q1</a> 寻求解决方案。</p>
<h2>可视化评估结果</h2>
<p>评估完成后，评估结果表格将打印如下：</p>
<pre lang="text" class="notranslate"><code class="notranslate">dataset    version    metric    mode      opt350m    opt125m
---------  ---------  --------  ------  ---------  ---------
siqa       e78df3     accuracy  gen         21.55      12.44
winograd   b6c7ed     accuracy  ppl         51.23      49.82
</code></pre>
<p>所有运行输出将定向到 <code class="notranslate">outputs/demo/</code> 目录，结构如下：</p>
<pre lang="text" class="notranslate"><code class="notranslate">outputs/default/
├── 20200220_120000
├── 20230220_183030     # 每个实验一个文件夹
│   ├── configs         # 用于记录的已转储的配置文件。如果在同一个实验文件夹中重新运行了不同的实验，可能会保留多个配置
│   ├── logs            # 推理和评估阶段的日志文件
│   │   ├── eval
│   │   └── infer
│   ├── predictions   # 每个任务的推理结果
│   ├── results       # 每个任务的评估结果
│   └── summary       # 单个实验的汇总评估结果
├── ...
</code></pre>
<p>打印评测结果的过程可被进一步定制化，用于输出一些数据集的平均分 (例如 MMLU, C-Eval 等)。</p>
<p>关于评测结果输出的更多介绍可阅读 <a href="../user_guides/summarizer.md">结果展示</a>。</p>
<h2>更多教程</h2>
<p>想要更多了解 OpenCompass, 可以点击下列链接学习。</p>
<ul>
<li><a href="https://opencompass.readthedocs.io/zh-cn/latest/" rel="nofollow">https://opencompass.readthedocs.io/zh-cn/latest/</a></li>
</ul>
<h2>作业</h2>
<p><strong>基础作业</strong></p>
<ul>
<li>使用 OpenCompass 评测 InternLM2-Chat-7B 模型在 C-Eval 数据集上的性能</li>
<li></li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/0d9a068a-8d04-46ee-a03b-9d42e5aa1da2"><img src="https://github.com/mattheliu/gitblog/assets/102272920/0d9a068a-8d04-46ee-a03b-9d42e5aa1da2" alt="M MO~~R5CAMMV7IOVS258" style="max-width: 100%;"></a></p>
<p><strong>进阶作业</strong></p>
<ul>
<li>使用 OpenCompass 评测 InternLM2-Chat-7B 模型使用 LMDeploy 0.2.0 部署后在 C-Eval 数据集上的性能</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/7845bc9c-56b1-45b0-80fa-26b2b2c21306"><img width="657" alt="屏幕截图 2024-01-28 192039" src="https://github.com/mattheliu/gitblog/assets/102272920/7845bc9c-56b1-45b0-80fa-26b2b2c21306" style="max-width: 100%;"></a></p>
<p>备注：<strong>由于进阶作业较难，完成基础作业之后就可以先提交作业了，在后续的大作业项目中使用这些技术将作为重要的加分点！</strong></p>
<p><strong>整体实训营项目：</strong></p>
<p>时间周期：即日起致课程结束</p>
<p>即日开始可以在班级群中随机组队完成一个大作业项目，一些可提供的选题如下：</p>
<ul>
<li>人情世故大模型：一个帮助用户撰写新年祝福文案的人情事故大模型</li>
<li>中小学数学大模型：一个拥有一定数学解题能力的大模型</li>
<li>心理大模型：一个治愈的心理大模型</li>
<li>工具调用类项目：结合 Lagent 构建数据集训练 InternLM 模型，支持对 MMYOLO 等工具的调用</li>
</ul>
<p>其他基于书生·浦语工具链的小项目都在范围内，欢迎大家充分发挥想象力。</p>
</div>
<div style="font-size:small;margin-top:8px;float:right;"></div>
<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>
</div>
    <div id="footer">Copyright © <span id="year"></span><a href="https://mattheliu.github.io"> Leonliuzx-Blog </a>
<p>
<span id="runday"></span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a>
</p>

<script>
if(""!=""){
    var now=new Date();
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("year").innerHTML=now.getFullYear();
    if(""!=""){document.getElementById("runday").innerHTML=" • "+"网站运行"+diffDay+"天"+" • ";}
    else{document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";}
}
</script>
</div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n\n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);

function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","mattheliu/mattheliu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
