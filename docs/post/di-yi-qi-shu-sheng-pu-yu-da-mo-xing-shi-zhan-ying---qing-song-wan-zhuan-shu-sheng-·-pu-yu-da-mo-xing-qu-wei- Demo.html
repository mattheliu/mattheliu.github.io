<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="## 1 大模型及 InternLM 模型简介

### 1.1 什么是大模型？

  大模型通常指的是机器学习或人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。">
<meta property="og:title" content="第一期书生浦语大模型实战营-轻松玩转书生·浦语大模型趣味 Demo">
<meta property="og:description" content="## 1 大模型及 InternLM 模型简介

### 1.1 什么是大模型？

  大模型通常指的是机器学习或人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---qing-song-wan-zhuan-shu-sheng-%C2%B7-pu-yu-da-mo-xing-qu-wei-%20Demo.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>第一期书生浦语大模型实战营-轻松玩转书生·浦语大模型趣味 Demo</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">第一期书生浦语大模型实战营-轻松玩转书生·浦语大模型趣味 Demo</h1>
<div class="title-right">
    <a href="https://mattheliu.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/mattheliu/mattheliu.github.io/issues/2" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h2>1 大模型及 InternLM 模型简介</h2>
<h3>1.1 什么是大模型？</h3>
<p>  大模型通常指的是机器学习或人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。这些模型利用大量数据进行训练，并且拥有数十亿甚至数千亿个参数。大模型的出现和发展得益于增长的数据量、计算能力的提升以及算法优化等因素。这些模型在各种任务中展现出惊人的性能，比如自然语言处理、计算机视觉、语音识别等。这种模型通常采用深度神经网络结构，如 <code class="notranslate">Transformer</code>、<code class="notranslate">BERT</code>、<code class="notranslate">GPT</code>（ Generative Pre-trained Transformer ）等。</p>
<p>  大模型的优势在于其能够捕捉和理解数据中更为复杂、抽象的特征和关系。通过大规模参数的学习，它们可以提高在各种任务上的泛化能力，并在未经过大量特定领域数据训练的情况下实现较好的表现。然而，大模型也面临着一些挑战，比如巨大的计算资源需求、高昂的训练成本、对大规模数据的依赖以及模型的可解释性等问题。因此，大模型的应用和发展也需要在性能、成本和道德等多个方面进行权衡和考量。</p>
<h3>1.2 InternLM 模型全链条开源</h3>
<p>  <code class="notranslate">InternLM</code> 是一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖。通过单一的代码库，它支持在拥有数千个 <code class="notranslate">GPU</code> 的大型集群上进行预训练，并在单个 <code class="notranslate">GPU</code> 上进行微调，同时实现了卓越的性能优化。在 <code class="notranslate">1024</code> 个 <code class="notranslate">GPU</code> 上训练时，<code class="notranslate">InternLM</code> 可以实现近 <code class="notranslate">90%</code> 的加速效率。</p>
<p>  基于 <code class="notranslate">InternLM</code> 训练框架，上海人工智能实验室已经发布了两个开源的预训练模型：<code class="notranslate">InternLM-7B</code> 和 <code class="notranslate">InternLM-20B</code>。</p>
<p>  <code class="notranslate">Lagent</code> 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 <code class="notranslate">Lagent</code> 框架可以更好的发挥 <code class="notranslate">InternLM</code> 的全部性能。</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/Lagent.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/Lagent.png" alt="Lagent 框架图" style="max-width: 100%;"></a></p>
<p>  浦语·灵笔是基于书生·浦语大语言模型研发的视觉-语言大模型，提供出色的图文理解和创作能力，结合了视觉和语言的先进技术，能够实现图像到文本、文本到图像的双向转换。使用浦语·灵笔大模型可以轻松的创作一篇图文推文，也能够轻松识别一张图片中的物体，并生成对应的文本描述。</p>
<p>  上述提到的所有模型，都会带领大家一起体验哦！欢迎大家来给 <code class="notranslate">InternLM</code>: <a href="https://github.com/InternLM/InternLM/">https://github.com/InternLM/InternLM/</a> 点点 star 哦！</p>
<h2>2 InternLM-Chat-7B 智能对话 Demo</h2>
<p>本小节我们将使用 <a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 中的 A100(1/4) 机器和 <code class="notranslate">InternLM-Chat-7B</code> 模型部署一个智能对话 Demo。</p>
<h3>2.1 环境准备</h3>
<p>在 <a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 平台中选择 A100(1/4) 的配置，如下图所示镜像选择 <code class="notranslate">Cuda11.7-conda</code>，如下图所示：</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>接下来打开刚刚租用服务器的<code class="notranslate">进入开发机</code>，并且打开其中的终端开始环境配置、模型下载和运行 <code class="notranslate">demo</code>。</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-1.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-1.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>进入开发机后，在页面的左上角可以切换 <code class="notranslate">JupyterLab</code>、<code class="notranslate">终端</code>和 <code class="notranslate">VScode</code>，并在终端输入 <code class="notranslate">bash 命令，进入</code> conda` 环境。如下图所示：</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-11.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-11.png" alt="Alt text" style="max-width: 100%;"></a><br>
进入 <code class="notranslate">conda</code> 环境之后，使用以下命令从本地克隆一个已有的 <code class="notranslate">pytorch 2.0.1</code> 的环境</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">bash <span class="pl-c"><span class="pl-c">#</span> 请每次使用 jupyter lab 打开终端时务必先执行 bash 命令进入 bash 中</span>
conda create --name internlm-demo --clone=/root/share/conda_envs/internlm-base</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/2b1e9904-aba3-4082-ac98-b006bd470857"><img src="https://github.com/mattheliu/gitblog/assets/102272920/2b1e9904-aba3-4082-ac98-b006bd470857" alt="Pasted image 20240107103927" style="max-width: 100%;"></a><br>
然后使用以下命令激活环境</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">conda activate internlm-demo</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/6d7856bd-f651-4466-8b04-8553e64a06f2"><img src="https://github.com/mattheliu/gitblog/assets/102272920/6d7856bd-f651-4466-8b04-8553e64a06f2" alt="Pasted image 20240107104048" style="max-width: 100%;"></a><br>
并在环境中安装运行 demo 所需要的依赖。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 升级pip</span>
python -m pip install --upgrade pip

pip install modelscope==1.9.5
pip install transformers==4.35.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/6f27c5a6-3831-43bd-804f-5a26a51cb6bc"><img src="https://github.com/mattheliu/gitblog/assets/102272920/6f27c5a6-3831-43bd-804f-5a26a51cb6bc" alt="Pasted image 20240107104327" style="max-width: 100%;"></a></p>
<h3>2.2 模型下载</h3>
<p><a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 平台的 <code class="notranslate">share</code> 目录下已经为我们准备了全系列的 <code class="notranslate">InternLM</code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">mkdir -p /root/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory</pre></div>
<blockquote>
<p>-r 选项表示递归地复制目录及其内容</p>
</blockquote>
<p>也可以使用 <code class="notranslate">modelscope</code> 中的 <code class="notranslate">snapshot_download</code> 函数下载模型，第一个参数为模型名称，参数 <code class="notranslate">cache_dir</code> 为模型的下载路径。</p>
<p>在 <code class="notranslate">/root</code> 路径下新建目录 <code class="notranslate">model</code>，在目录下新建 <code class="notranslate">download.py</code> 文件并在其中输入以下内容，粘贴代码后记得保存文件，如下图所示。并运行 <code class="notranslate">python /root/model/download.py</code> 执行下载，模型大小为 14 GB，下载模型大概需要 10~20 分钟</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">modelscope</span> <span class="pl-k">import</span> <span class="pl-s1">snapshot_download</span>, <span class="pl-v">AutoModel</span>, <span class="pl-v">AutoTokenizer</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-en">snapshot_download</span>(<span class="pl-s">'Shanghai_AI_Laboratory/internlm-chat-7b'</span>, <span class="pl-s1">cache_dir</span><span class="pl-c1">=</span><span class="pl-s">'/root/model'</span>, <span class="pl-s1">revision</span><span class="pl-c1">=</span><span class="pl-s">'v1.0.3'</span>)</pre></div>
<blockquote>
<p>注意：使用 <code class="notranslate">pwd</code> 命令可以查看当前的路径，<code class="notranslate">JupyterLab</code> 左侧目录栏显示为 <code class="notranslate">/root/</code> 下的路径。</p>
</blockquote>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-2.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-2.png" alt="image" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/6aaf0239-f64e-4d27-8d36-097e4eac4e24"><img src="https://github.com/mattheliu/gitblog/assets/102272920/6aaf0239-f64e-4d27-8d36-097e4eac4e24" alt="Pasted image 20240107104625" style="max-width: 100%;"></a></p>
<h3>2.3 代码准备</h3>
<p>首先 <code class="notranslate">clone</code> 代码，在 <code class="notranslate">/root</code> 路径下新建 <code class="notranslate">code</code> 目录，然后切换路径, clone 代码.</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c1">cd</span> /root/code
git clone https://gitee.com/internlm/InternLM.git</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/03a45227-ac66-4569-ba8a-1f9d60d87f8d"><img src="https://github.com/mattheliu/gitblog/assets/102272920/03a45227-ac66-4569-ba8a-1f9d60d87f8d" alt="Pasted image 20240107104830" style="max-width: 100%;"></a><br>
切换 commit 版本，与教程 commit 版本保持一致，可以让大家更好的复现。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c1">cd</span> InternLM
git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/2943aefd-a0c0-458d-82e3-c88e8562740a"><img src="https://github.com/mattheliu/gitblog/assets/102272920/2943aefd-a0c0-458d-82e3-c88e8562740a" alt="Pasted image 20240107104955" style="max-width: 100%;"></a><br>
将 <code class="notranslate">/root/code/InternLM/web_demo.py</code> 中 29 行和 33 行的模型更换为本地的 <code class="notranslate">/root/model/Shanghai_AI_Laboratory/internlm-chat-7b</code>。</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-3.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-3.png" alt="image-3" style="max-width: 100%;"></a></p>
<h3>2.4 终端运行</h3>
<p>我们可以在 <code class="notranslate">/root/code/InternLM</code> 目录下新建一个 <code class="notranslate">cli_demo.py</code> 文件，将以下代码填入其中：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoTokenizer</span>, <span class="pl-v">AutoModelForCausalLM</span>


<span class="pl-s1">model_name_or_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"</span>

<span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">AutoTokenizer</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s1">model_name_or_path</span>, <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModelForCausalLM</span>.<span class="pl-en">from_pretrained</span>(<span class="pl-s1">model_name_or_path</span>, <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>, <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-s1">bfloat16</span>, <span class="pl-s1">device_map</span><span class="pl-c1">=</span><span class="pl-s">'auto'</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">eval</span>()

<span class="pl-s1">system_prompt</span> <span class="pl-c1">=</span> <span class="pl-s">"""You are an AI assistant whose name is InternLM (书生·浦语).</span>
<span class="pl-s">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span>
<span class="pl-s">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span>
<span class="pl-s">"""</span>

<span class="pl-s1">messages</span> <span class="pl-c1">=</span> [(<span class="pl-s1">system_prompt</span>, <span class="pl-s">''</span>)]

<span class="pl-en">print</span>(<span class="pl-s">"=============Welcome to InternLM chatbot, type 'exit' to exit.============="</span>)

<span class="pl-k">while</span> <span class="pl-c1">True</span>:
    <span class="pl-s1">input_text</span> <span class="pl-c1">=</span> <span class="pl-en">input</span>(<span class="pl-s">"User  &gt;&gt;&gt; "</span>)
    <span class="pl-s1">input_text</span> <span class="pl-c1">=</span> <span class="pl-s1">input_text</span>.<span class="pl-en">replace</span>(<span class="pl-s">' '</span>, <span class="pl-s">''</span>)
    <span class="pl-k">if</span> <span class="pl-s1">input_text</span> <span class="pl-c1">==</span> <span class="pl-s">"exit"</span>:
        <span class="pl-k">break</span>
    <span class="pl-s1">response</span>, <span class="pl-s1">history</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-en">chat</span>(<span class="pl-s1">tokenizer</span>, <span class="pl-s1">input_text</span>, <span class="pl-s1">history</span><span class="pl-c1">=</span><span class="pl-s1">messages</span>)
    <span class="pl-s1">messages</span>.<span class="pl-en">append</span>((<span class="pl-s1">input_text</span>, <span class="pl-s1">response</span>))
    <span class="pl-en">print</span>(<span class="pl-s">f"robot &gt;&gt;&gt; <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">response</span><span class="pl-kos">}</span></span>"</span>)</pre></div>
<p>然后在终端运行以下命令，即可体验 <code class="notranslate">InternLM-Chat-7B</code> 模型的对话能力。对话效果如下所示：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python /root/code/InternLM/cli_demo.py</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/f75e3521-747d-4c7d-b527-7456116744db"><img src="https://github.com/mattheliu/gitblog/assets/102272920/f75e3521-747d-4c7d-b527-7456116744db" alt="Pasted image 20240107105513" style="max-width: 100%;"></a></p>
<h3>2.5 web demo 运行</h3>
<p>我们切换到 <code class="notranslate">VScode</code> 中，运行 <code class="notranslate">/root/code/InternLM</code> 目录下的 <code class="notranslate">web_demo.py</code> 文件，输入以下命令后，<a href="https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3"><strong>查看本教程5.2配置本地端口后</strong></a>，将端口映射到本地。在本地浏览器输入 <code class="notranslate">http://127.0.0.1:6006</code> 即可。</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">bash
conda activate internlm-demo  <span class="pl-c"><span class="pl-c">#</span> 首次进入 vscode 会默认是 base 环境，所以首先切换环境</span>
<span class="pl-c1">cd</span> /root/code/InternLM
streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006</pre></div>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-12.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-12.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>注意：要在浏览器打开 <code class="notranslate">http://127.0.0.1:6006</code> 页面后，模型才会加载，如下图所示：</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-5.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-5.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>在加载完模型之后，就可以与 InternLM-Chat-7B 进行对话了，如下图所示：</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/6fa9d061-eedd-4b08-ad4e-40f4c45a8113"><img src="https://github.com/mattheliu/gitblog/assets/102272920/6fa9d061-eedd-4b08-ad4e-40f4c45a8113" alt="Pasted image 20240107110914" style="max-width: 100%;"></a></p>
<h2>3 Lagent 智能体工具调用 Demo</h2>
<p>本小节我们将使用 <a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 中的 A100(1/4) 机器、<code class="notranslate">InternLM-Chat-7B</code> 模型和 <code class="notranslate">Lagent</code> 框架部署一个智能工具调用 Demo。</p>
<p>Lagent 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 Lagent 框架可以更好的发挥 InternLM 的全部性能。</p>
<p>下面我们就开始动手实现！</p>
<h3>3.1 环境准备</h3>
<p>选择和第一个 <code class="notranslate">InternLM</code> 一样的镜像环境，运行以下命令安装依赖，如果上一个 <code class="notranslate">InternLM-Chat-7B</code> 已经配置好环境不需要重复安装.</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c"><span class="pl-c">#</span> 升级pip</span>
python -m pip install --upgrade pip

pip install modelscope==1.9.5
pip install transformers==4.35.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1</pre></div>
<h3>3.2 模型下载</h3>
<p><a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 平台的 <code class="notranslate">share</code> 目录下已经为我们准备了全系列的 <code class="notranslate">InternLM</code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">mkdir -p /root/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory</pre></div>
<blockquote>
<p>-r 选项表示递归地复制目录及其内容</p>
</blockquote>
<p>也可以在 <code class="notranslate">/root/model</code> 路径下新建 <code class="notranslate">download.py</code> 文件并在其中输入以下内容，并运行 <code class="notranslate">python /root/model/download.py</code> 执行下载，模型大小为 14 GB，下载模型大概需要 10~20 分钟</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">modelscope</span> <span class="pl-k">import</span> <span class="pl-s1">snapshot_download</span>, <span class="pl-v">AutoModel</span>, <span class="pl-v">AutoTokenizer</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-en">snapshot_download</span>(<span class="pl-s">'Shanghai_AI_Laboratory/internlm-chat-7b'</span>, <span class="pl-s1">cache_dir</span><span class="pl-c1">=</span><span class="pl-s">'/root/model'</span>, <span class="pl-s1">revision</span><span class="pl-c1">=</span><span class="pl-s">'v1.0.3'</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/dc56d616-565a-4d63-95fd-4bf5c3b18593"><img src="https://github.com/mattheliu/gitblog/assets/102272920/dc56d616-565a-4d63-95fd-4bf5c3b18593" alt="Pasted image 20240107111341" style="max-width: 100%;"></a></p>
<h3>3.3 Lagent 安装</h3>
<p>首先切换路径到 <code class="notranslate">/root/code</code> 克隆 <code class="notranslate">lagent</code> 仓库，并通过 <code class="notranslate">pip install -e .</code> 源码安装 <code class="notranslate">Lagent</code></p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c1">cd</span> /root/code
git clone https://gitee.com/internlm/lagent.git
<span class="pl-c1">cd</span> /root/code/lagent
git checkout 511b03889010c4811b1701abb153e02b8e94fb5e <span class="pl-c"><span class="pl-c">#</span> 尽量保证和教程commit版本一致</span>
pip install -e <span class="pl-c1">.</span> <span class="pl-c"><span class="pl-c">#</span> 源码安装</span></pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/89e00890-8f17-4c02-8363-df9712b54e86"><img src="https://github.com/mattheliu/gitblog/assets/102272920/89e00890-8f17-4c02-8363-df9712b54e86" alt="Pasted image 20240107111512" style="max-width: 100%;"></a></p>
<h3>3.4 修改代码</h3>
<p>由于代码修改的地方比较多，大家直接将 <code class="notranslate">/root/code/lagent/examples/react_web_demo.py</code> 内容替换为以下代码</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">copy</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>

<span class="pl-k">import</span> <span class="pl-s1">streamlit</span> <span class="pl-k">as</span> <span class="pl-s1">st</span>
<span class="pl-k">from</span> <span class="pl-s1">streamlit</span>.<span class="pl-s1">logger</span> <span class="pl-k">import</span> <span class="pl-s1">get_logger</span>

<span class="pl-k">from</span> <span class="pl-s1">lagent</span>.<span class="pl-s1">actions</span> <span class="pl-k">import</span> <span class="pl-v">ActionExecutor</span>, <span class="pl-v">GoogleSearch</span>, <span class="pl-v">PythonInterpreter</span>
<span class="pl-k">from</span> <span class="pl-s1">lagent</span>.<span class="pl-s1">agents</span>.<span class="pl-s1">react</span> <span class="pl-k">import</span> <span class="pl-v">ReAct</span>
<span class="pl-k">from</span> <span class="pl-s1">lagent</span>.<span class="pl-s1">llms</span> <span class="pl-k">import</span> <span class="pl-v">GPTAPI</span>
<span class="pl-k">from</span> <span class="pl-s1">lagent</span>.<span class="pl-s1">llms</span>.<span class="pl-s1">huggingface</span> <span class="pl-k">import</span> <span class="pl-v">HFTransformerCasualLM</span>


<span class="pl-k">class</span> <span class="pl-v">SessionState</span>:

    <span class="pl-k">def</span> <span class="pl-en">init_state</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""Initialize session state variables."""</span>
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'assistant'</span>] <span class="pl-c1">=</span> []
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'user'</span>] <span class="pl-c1">=</span> []

        <span class="pl-c">#action_list = [PythonInterpreter(), GoogleSearch()]</span>
        <span class="pl-s1">action_list</span> <span class="pl-c1">=</span> [<span class="pl-v">PythonInterpreter</span>()]
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'plugin_map'</span>] <span class="pl-c1">=</span> {
            <span class="pl-s1">action</span>.<span class="pl-s1">name</span>: <span class="pl-s1">action</span>
            <span class="pl-k">for</span> <span class="pl-s1">action</span> <span class="pl-c1">in</span> <span class="pl-s1">action_list</span>
        }
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>] <span class="pl-c1">=</span> {}
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_selected'</span>] <span class="pl-c1">=</span> <span class="pl-c1">None</span>
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'plugin_actions'</span>] <span class="pl-c1">=</span> <span class="pl-en">set</span>()

    <span class="pl-k">def</span> <span class="pl-en">clear_state</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""Clear the existing session state."""</span>
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'assistant'</span>] <span class="pl-c1">=</span> []
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'user'</span>] <span class="pl-c1">=</span> []
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_selected'</span>] <span class="pl-c1">=</span> <span class="pl-c1">None</span>
        <span class="pl-k">if</span> <span class="pl-s">'chatbot'</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>:
            <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'chatbot'</span>].<span class="pl-s1">_session_history</span> <span class="pl-c1">=</span> []


<span class="pl-k">class</span> <span class="pl-v">StreamlitUI</span>:

    <span class="pl-k">def</span> <span class="pl-en">__init__</span>(<span class="pl-s1">self</span>, <span class="pl-s1">session_state</span>: <span class="pl-v">SessionState</span>):
        <span class="pl-s1">self</span>.<span class="pl-en">init_streamlit</span>()
        <span class="pl-s1">self</span>.<span class="pl-s1">session_state</span> <span class="pl-c1">=</span> <span class="pl-s1">session_state</span>

    <span class="pl-k">def</span> <span class="pl-en">init_streamlit</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""Initialize Streamlit's UI settings."""</span>
        <span class="pl-s1">st</span>.<span class="pl-en">set_page_config</span>(
            <span class="pl-s1">layout</span><span class="pl-c1">=</span><span class="pl-s">'wide'</span>,
            <span class="pl-s1">page_title</span><span class="pl-c1">=</span><span class="pl-s">'lagent-web'</span>,
            <span class="pl-s1">page_icon</span><span class="pl-c1">=</span><span class="pl-s">'./docs/imgs/lagent_icon.png'</span>)
        <span class="pl-c"># st.header(':robot_face: :blue[Lagent] Web Demo ', divider='rainbow')</span>
        <span class="pl-s1">st</span>.<span class="pl-s1">sidebar</span>.<span class="pl-en">title</span>(<span class="pl-s">'模型控制'</span>)

    <span class="pl-k">def</span> <span class="pl-en">setup_sidebar</span>(<span class="pl-s1">self</span>):
        <span class="pl-s">"""Setup the sidebar for model and plugin selection."""</span>
        <span class="pl-s1">model_name</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">sidebar</span>.<span class="pl-en">selectbox</span>(
            <span class="pl-s">'模型选择：'</span>, <span class="pl-s1">options</span><span class="pl-c1">=</span>[<span class="pl-s">'gpt-3.5-turbo'</span>,<span class="pl-s">'internlm'</span>])
        <span class="pl-k">if</span> <span class="pl-s1">model_name</span> <span class="pl-c1">!=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_selected'</span>]:
            <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">self</span>.<span class="pl-en">init_model</span>(<span class="pl-s1">model_name</span>)
            <span class="pl-s1">self</span>.<span class="pl-s1">session_state</span>.<span class="pl-en">clear_state</span>()
            <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_selected'</span>] <span class="pl-c1">=</span> <span class="pl-s1">model_name</span>
            <span class="pl-k">if</span> <span class="pl-s">'chatbot'</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>:
                <span class="pl-k">del</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'chatbot'</span>]
        <span class="pl-k">else</span>:
            <span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>][<span class="pl-s1">model_name</span>]

        <span class="pl-s1">plugin_name</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">sidebar</span>.<span class="pl-en">multiselect</span>(
            <span class="pl-s">'插件选择'</span>,
            <span class="pl-s1">options</span><span class="pl-c1">=</span><span class="pl-en">list</span>(<span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'plugin_map'</span>].<span class="pl-en">keys</span>()),
            <span class="pl-s1">default</span><span class="pl-c1">=</span>[<span class="pl-en">list</span>(<span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'plugin_map'</span>].<span class="pl-en">keys</span>())[<span class="pl-c1">0</span>]],
        )

        <span class="pl-s1">plugin_action</span> <span class="pl-c1">=</span> [
            <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'plugin_map'</span>][<span class="pl-s1">name</span>] <span class="pl-k">for</span> <span class="pl-s1">name</span> <span class="pl-c1">in</span> <span class="pl-s1">plugin_name</span>
        ]
        <span class="pl-k">if</span> <span class="pl-s">'chatbot'</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>:
            <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'chatbot'</span>].<span class="pl-s1">_action_executor</span> <span class="pl-c1">=</span> <span class="pl-v">ActionExecutor</span>(
                <span class="pl-s1">actions</span><span class="pl-c1">=</span><span class="pl-s1">plugin_action</span>)
        <span class="pl-k">if</span> <span class="pl-s1">st</span>.<span class="pl-s1">sidebar</span>.<span class="pl-en">button</span>(<span class="pl-s">'清空对话'</span>, <span class="pl-s1">key</span><span class="pl-c1">=</span><span class="pl-s">'clear'</span>):
            <span class="pl-s1">self</span>.<span class="pl-s1">session_state</span>.<span class="pl-en">clear_state</span>()
        <span class="pl-s1">uploaded_file</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">sidebar</span>.<span class="pl-en">file_uploader</span>(
            <span class="pl-s">'上传文件'</span>, <span class="pl-s1">type</span><span class="pl-c1">=</span>[<span class="pl-s">'png'</span>, <span class="pl-s">'jpg'</span>, <span class="pl-s">'jpeg'</span>, <span class="pl-s">'mp4'</span>, <span class="pl-s">'mp3'</span>, <span class="pl-s">'wav'</span>])
        <span class="pl-k">return</span> <span class="pl-s1">model_name</span>, <span class="pl-s1">model</span>, <span class="pl-s1">plugin_action</span>, <span class="pl-s1">uploaded_file</span>

    <span class="pl-k">def</span> <span class="pl-en">init_model</span>(<span class="pl-s1">self</span>, <span class="pl-s1">option</span>):
        <span class="pl-s">"""Initialize the model based on the selected option."""</span>
        <span class="pl-k">if</span> <span class="pl-s1">option</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>]:
            <span class="pl-k">if</span> <span class="pl-s1">option</span>.<span class="pl-en">startswith</span>(<span class="pl-s">'gpt'</span>):
                <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>][<span class="pl-s1">option</span>] <span class="pl-c1">=</span> <span class="pl-v">GPTAPI</span>(
                    <span class="pl-s1">model_type</span><span class="pl-c1">=</span><span class="pl-s1">option</span>)
            <span class="pl-k">else</span>:
                <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>][<span class="pl-s1">option</span>] <span class="pl-c1">=</span> <span class="pl-v">HFTransformerCasualLM</span>(
                    <span class="pl-s">'/root/model/Shanghai_AI_Laboratory/internlm-chat-7b'</span>)
        <span class="pl-k">return</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'model_map'</span>][<span class="pl-s1">option</span>]

    <span class="pl-k">def</span> <span class="pl-en">initialize_chatbot</span>(<span class="pl-s1">self</span>, <span class="pl-s1">model</span>, <span class="pl-s1">plugin_action</span>):
        <span class="pl-s">"""Initialize the chatbot with the given model and plugin actions."""</span>
        <span class="pl-k">return</span> <span class="pl-v">ReAct</span>(
            <span class="pl-s1">llm</span><span class="pl-c1">=</span><span class="pl-s1">model</span>, <span class="pl-s1">action_executor</span><span class="pl-c1">=</span><span class="pl-v">ActionExecutor</span>(<span class="pl-s1">actions</span><span class="pl-c1">=</span><span class="pl-s1">plugin_action</span>))

    <span class="pl-k">def</span> <span class="pl-en">render_user</span>(<span class="pl-s1">self</span>, <span class="pl-s1">prompt</span>: <span class="pl-s1">str</span>):
        <span class="pl-k">with</span> <span class="pl-s1">st</span>.<span class="pl-en">chat_message</span>(<span class="pl-s">'user'</span>):
            <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(<span class="pl-s1">prompt</span>)

    <span class="pl-k">def</span> <span class="pl-en">render_assistant</span>(<span class="pl-s1">self</span>, <span class="pl-s1">agent_return</span>):
        <span class="pl-k">with</span> <span class="pl-s1">st</span>.<span class="pl-en">chat_message</span>(<span class="pl-s">'assistant'</span>):
            <span class="pl-k">for</span> <span class="pl-s1">action</span> <span class="pl-c1">in</span> <span class="pl-s1">agent_return</span>.<span class="pl-s1">actions</span>:
                <span class="pl-k">if</span> (<span class="pl-s1">action</span>):
                    <span class="pl-s1">self</span>.<span class="pl-en">render_action</span>(<span class="pl-s1">action</span>)
            <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(<span class="pl-s1">agent_return</span>.<span class="pl-s1">response</span>)

    <span class="pl-k">def</span> <span class="pl-en">render_action</span>(<span class="pl-s1">self</span>, <span class="pl-s1">action</span>):
        <span class="pl-k">with</span> <span class="pl-s1">st</span>.<span class="pl-en">expander</span>(<span class="pl-s1">action</span>.<span class="pl-s1">type</span>, <span class="pl-s1">expanded</span><span class="pl-c1">=</span><span class="pl-c1">True</span>):
            <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(
                <span class="pl-s">"&lt;p style='text-align: left;display:flex;'&gt; &lt;span style='font-size:14px;font-weight:600;width:70px;text-align-last: justify;'&gt;插    件&lt;/span&gt;&lt;span style='width:14px;text-align:left;display:block;'&gt;:&lt;/span&gt;&lt;span style='flex:1;'&gt;"</span>  <span class="pl-c"># noqa E501</span>
                <span class="pl-c1">+</span> <span class="pl-s1">action</span>.<span class="pl-s1">type</span> <span class="pl-c1">+</span> <span class="pl-s">'&lt;/span&gt;&lt;/p&gt;'</span>,
                <span class="pl-s1">unsafe_allow_html</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(
                <span class="pl-s">"&lt;p style='text-align: left;display:flex;'&gt; &lt;span style='font-size:14px;font-weight:600;width:70px;text-align-last: justify;'&gt;思考步骤&lt;/span&gt;&lt;span style='width:14px;text-align:left;display:block;'&gt;:&lt;/span&gt;&lt;span style='flex:1;'&gt;"</span>  <span class="pl-c"># noqa E501</span>
                <span class="pl-c1">+</span> <span class="pl-s1">action</span>.<span class="pl-s1">thought</span> <span class="pl-c1">+</span> <span class="pl-s">'&lt;/span&gt;&lt;/p&gt;'</span>,
                <span class="pl-s1">unsafe_allow_html</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-k">if</span> (<span class="pl-en">isinstance</span>(<span class="pl-s1">action</span>.<span class="pl-s1">args</span>, <span class="pl-s1">dict</span>) <span class="pl-c1">and</span> <span class="pl-s">'text'</span> <span class="pl-c1">in</span> <span class="pl-s1">action</span>.<span class="pl-s1">args</span>):
                <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(
                    <span class="pl-s">"&lt;p style='text-align: left;display:flex;'&gt;&lt;span style='font-size:14px;font-weight:600;width:70px;text-align-last: justify;'&gt; 执行内容&lt;/span&gt;&lt;span style='width:14px;text-align:left;display:block;'&gt;:&lt;/span&gt;&lt;/p&gt;"</span>,  <span class="pl-c"># noqa E501</span>
                    <span class="pl-s1">unsafe_allow_html</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
                <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(<span class="pl-s1">action</span>.<span class="pl-s1">args</span>[<span class="pl-s">'text'</span>])
            <span class="pl-s1">self</span>.<span class="pl-en">render_action_results</span>(<span class="pl-s1">action</span>)

    <span class="pl-k">def</span> <span class="pl-en">render_action_results</span>(<span class="pl-s1">self</span>, <span class="pl-s1">action</span>):
        <span class="pl-s">"""Render the results of action, including text, images, videos, and</span>
<span class="pl-s">        audios."""</span>
        <span class="pl-k">if</span> (<span class="pl-en">isinstance</span>(<span class="pl-s1">action</span>.<span class="pl-s1">result</span>, <span class="pl-s1">dict</span>)):
            <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(
                <span class="pl-s">"&lt;p style='text-align: left;display:flex;'&gt;&lt;span style='font-size:14px;font-weight:600;width:70px;text-align-last: justify;'&gt; 执行结果&lt;/span&gt;&lt;span style='width:14px;text-align:left;display:block;'&gt;:&lt;/span&gt;&lt;/p&gt;"</span>,  <span class="pl-c"># noqa E501</span>
                <span class="pl-s1">unsafe_allow_html</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-k">if</span> <span class="pl-s">'text'</span> <span class="pl-c1">in</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>:
                <span class="pl-s1">st</span>.<span class="pl-en">markdown</span>(
                    <span class="pl-s">"&lt;p style='text-align: left;'&gt;"</span> <span class="pl-c1">+</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>[<span class="pl-s">'text'</span>] <span class="pl-c1">+</span>
                    <span class="pl-s">'&lt;/p&gt;'</span>,
                    <span class="pl-s1">unsafe_allow_html</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
            <span class="pl-k">if</span> <span class="pl-s">'image'</span> <span class="pl-c1">in</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>:
                <span class="pl-s1">image_path</span> <span class="pl-c1">=</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>[<span class="pl-s">'image'</span>]
                <span class="pl-s1">image_data</span> <span class="pl-c1">=</span> <span class="pl-en">open</span>(<span class="pl-s1">image_path</span>, <span class="pl-s">'rb'</span>).<span class="pl-en">read</span>()
                <span class="pl-s1">st</span>.<span class="pl-en">image</span>(<span class="pl-s1">image_data</span>, <span class="pl-s1">caption</span><span class="pl-c1">=</span><span class="pl-s">'Generated Image'</span>)
            <span class="pl-k">if</span> <span class="pl-s">'video'</span> <span class="pl-c1">in</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>:
                <span class="pl-s1">video_data</span> <span class="pl-c1">=</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>[<span class="pl-s">'video'</span>]
                <span class="pl-s1">video_data</span> <span class="pl-c1">=</span> <span class="pl-en">open</span>(<span class="pl-s1">video_data</span>, <span class="pl-s">'rb'</span>).<span class="pl-en">read</span>()
                <span class="pl-s1">st</span>.<span class="pl-en">video</span>(<span class="pl-s1">video_data</span>)
            <span class="pl-k">if</span> <span class="pl-s">'audio'</span> <span class="pl-c1">in</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>:
                <span class="pl-s1">audio_data</span> <span class="pl-c1">=</span> <span class="pl-s1">action</span>.<span class="pl-s1">result</span>[<span class="pl-s">'audio'</span>]
                <span class="pl-s1">audio_data</span> <span class="pl-c1">=</span> <span class="pl-en">open</span>(<span class="pl-s1">audio_data</span>, <span class="pl-s">'rb'</span>).<span class="pl-en">read</span>()
                <span class="pl-s1">st</span>.<span class="pl-en">audio</span>(<span class="pl-s1">audio_data</span>)


<span class="pl-k">def</span> <span class="pl-en">main</span>():
    <span class="pl-s1">logger</span> <span class="pl-c1">=</span> <span class="pl-en">get_logger</span>(<span class="pl-s1">__name__</span>)
    <span class="pl-c"># Initialize Streamlit UI and setup sidebar</span>
    <span class="pl-k">if</span> <span class="pl-s">'ui'</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>:
        <span class="pl-s1">session_state</span> <span class="pl-c1">=</span> <span class="pl-v">SessionState</span>()
        <span class="pl-s1">session_state</span>.<span class="pl-en">init_state</span>()
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'ui'</span>] <span class="pl-c1">=</span> <span class="pl-v">StreamlitUI</span>(<span class="pl-s1">session_state</span>)

    <span class="pl-k">else</span>:
        <span class="pl-s1">st</span>.<span class="pl-en">set_page_config</span>(
            <span class="pl-s1">layout</span><span class="pl-c1">=</span><span class="pl-s">'wide'</span>,
            <span class="pl-s1">page_title</span><span class="pl-c1">=</span><span class="pl-s">'lagent-web'</span>,
            <span class="pl-s1">page_icon</span><span class="pl-c1">=</span><span class="pl-s">'./docs/imgs/lagent_icon.png'</span>)
        <span class="pl-c"># st.header(':robot_face: :blue[Lagent] Web Demo ', divider='rainbow')</span>
    <span class="pl-s1">model_name</span>, <span class="pl-s1">model</span>, <span class="pl-s1">plugin_action</span>, <span class="pl-s1">uploaded_file</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[
        <span class="pl-s">'ui'</span>].<span class="pl-en">setup_sidebar</span>()

    <span class="pl-c"># Initialize chatbot if it is not already initialized</span>
    <span class="pl-c"># or if the model has changed</span>
    <span class="pl-k">if</span> <span class="pl-s">'chatbot'</span> <span class="pl-c1">not</span> <span class="pl-c1">in</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span> <span class="pl-c1">or</span> <span class="pl-s1">model</span> <span class="pl-c1">!=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[
            <span class="pl-s">'chatbot'</span>].<span class="pl-s1">_llm</span>:
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'chatbot'</span>] <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[
            <span class="pl-s">'ui'</span>].<span class="pl-en">initialize_chatbot</span>(<span class="pl-s1">model</span>, <span class="pl-s1">plugin_action</span>)

    <span class="pl-k">for</span> <span class="pl-s1">prompt</span>, <span class="pl-s1">agent_return</span> <span class="pl-c1">in</span> <span class="pl-en">zip</span>(<span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'user'</span>],
                                    <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'assistant'</span>]):
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'ui'</span>].<span class="pl-en">render_user</span>(<span class="pl-s1">prompt</span>)
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'ui'</span>].<span class="pl-en">render_assistant</span>(<span class="pl-s1">agent_return</span>)
    <span class="pl-c"># User input form at the bottom (this part will be at the bottom)</span>
    <span class="pl-c"># with st.form(key='my_form', clear_on_submit=True):</span>

    <span class="pl-k">if</span> <span class="pl-s1">user_input</span> <span class="pl-c1">:=</span> <span class="pl-s1">st</span>.<span class="pl-en">chat_input</span>(<span class="pl-s">''</span>):
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'ui'</span>].<span class="pl-en">render_user</span>(<span class="pl-s1">user_input</span>)
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'user'</span>].<span class="pl-en">append</span>(<span class="pl-s1">user_input</span>)
        <span class="pl-c"># Add file uploader to sidebar</span>
        <span class="pl-k">if</span> <span class="pl-s1">uploaded_file</span>:
            <span class="pl-s1">file_bytes</span> <span class="pl-c1">=</span> <span class="pl-s1">uploaded_file</span>.<span class="pl-en">read</span>()
            <span class="pl-s1">file_type</span> <span class="pl-c1">=</span> <span class="pl-s1">uploaded_file</span>.<span class="pl-s1">type</span>
            <span class="pl-k">if</span> <span class="pl-s">'image'</span> <span class="pl-c1">in</span> <span class="pl-s1">file_type</span>:
                <span class="pl-s1">st</span>.<span class="pl-en">image</span>(<span class="pl-s1">file_bytes</span>, <span class="pl-s1">caption</span><span class="pl-c1">=</span><span class="pl-s">'Uploaded Image'</span>)
            <span class="pl-k">elif</span> <span class="pl-s">'video'</span> <span class="pl-c1">in</span> <span class="pl-s1">file_type</span>:
                <span class="pl-s1">st</span>.<span class="pl-en">video</span>(<span class="pl-s1">file_bytes</span>, <span class="pl-s1">caption</span><span class="pl-c1">=</span><span class="pl-s">'Uploaded Video'</span>)
            <span class="pl-k">elif</span> <span class="pl-s">'audio'</span> <span class="pl-c1">in</span> <span class="pl-s1">file_type</span>:
                <span class="pl-s1">st</span>.<span class="pl-en">audio</span>(<span class="pl-s1">file_bytes</span>, <span class="pl-s1">caption</span><span class="pl-c1">=</span><span class="pl-s">'Uploaded Audio'</span>)
            <span class="pl-c"># Save the file to a temporary location and get the path</span>
            <span class="pl-s1">file_path</span> <span class="pl-c1">=</span> <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s1">root_dir</span>, <span class="pl-s1">uploaded_file</span>.<span class="pl-s1">name</span>)
            <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s1">file_path</span>, <span class="pl-s">'wb'</span>) <span class="pl-k">as</span> <span class="pl-s1">tmpfile</span>:
                <span class="pl-s1">tmpfile</span>.<span class="pl-en">write</span>(<span class="pl-s1">file_bytes</span>)
            <span class="pl-s1">st</span>.<span class="pl-en">write</span>(<span class="pl-s">f'File saved at: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">file_path</span><span class="pl-kos">}</span></span>'</span>)
            <span class="pl-s1">user_input</span> <span class="pl-c1">=</span> <span class="pl-s">'我上传了一个图像，路径为: {file_path}. {user_input}'</span>.<span class="pl-en">format</span>(
                <span class="pl-s1">file_path</span><span class="pl-c1">=</span><span class="pl-s1">file_path</span>, <span class="pl-s1">user_input</span><span class="pl-c1">=</span><span class="pl-s1">user_input</span>)
        <span class="pl-s1">agent_return</span> <span class="pl-c1">=</span> <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'chatbot'</span>].<span class="pl-en">chat</span>(<span class="pl-s1">user_input</span>)
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'assistant'</span>].<span class="pl-en">append</span>(<span class="pl-s1">copy</span>.<span class="pl-en">deepcopy</span>(<span class="pl-s1">agent_return</span>))
        <span class="pl-s1">logger</span>.<span class="pl-en">info</span>(<span class="pl-s1">agent_return</span>.<span class="pl-s1">inner_steps</span>)
        <span class="pl-s1">st</span>.<span class="pl-s1">session_state</span>[<span class="pl-s">'ui'</span>].<span class="pl-en">render_assistant</span>(<span class="pl-s1">agent_return</span>)


<span class="pl-k">if</span> <span class="pl-s1">__name__</span> <span class="pl-c1">==</span> <span class="pl-s">'__main__'</span>:
    <span class="pl-s1">root_dir</span> <span class="pl-c1">=</span> <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">dirname</span>(<span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">dirname</span>(<span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">abspath</span>(<span class="pl-s1">__file__</span>)))
    <span class="pl-s1">root_dir</span> <span class="pl-c1">=</span> <span class="pl-s1">os</span>.<span class="pl-s1">path</span>.<span class="pl-en">join</span>(<span class="pl-s1">root_dir</span>, <span class="pl-s">'tmp_dir'</span>)
    <span class="pl-s1">os</span>.<span class="pl-en">makedirs</span>(<span class="pl-s1">root_dir</span>, <span class="pl-s1">exist_ok</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
    <span class="pl-en">main</span>()</pre></div>
<h3>3.5 Demo 运行</h3>
<div class="highlight highlight-source-shell"><pre class="notranslate">streamlit run /root/code/lagent/examples/react_web_demo.py --server.address 127.0.0.1 --server.port 6006</pre></div>
<p>用同样的方法我们依然切换到 <code class="notranslate">VScode</code> 页面，运行成功后，<a href="https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3"><strong>查看本教程5.2配置本地端口后</strong></a>，将端口映射到本地。在本地浏览器输入 <code class="notranslate">http://127.0.0.1:6006</code> 即可。</p>
<p>我们在 <code class="notranslate">Web</code> 页面选择 <code class="notranslate">InternLM</code> 模型，等待模型加载完毕后，输入数学问题 已知 <code class="notranslate">2x+3=10</code>，求<code class="notranslate">x</code> ,此时 <code class="notranslate">InternLM-Chat-7B</code> 模型理解题意生成解此题的 <code class="notranslate">Python</code> 代码，<code class="notranslate">Lagent</code> 调度送入 <code class="notranslate">Python</code> 代码解释器求出该问题的解。</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/a54dd401-5f43-4c5d-b959-bd720bf73fd2"><img src="https://github.com/mattheliu/gitblog/assets/102272920/a54dd401-5f43-4c5d-b959-bd720bf73fd2" alt="Pasted image 20240107113802" style="max-width: 100%;"></a></p>
<h2>4. 浦语·灵笔图文理解创作 Demo</h2>
<p>本小节我们将使用 <a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 中的 A100(1/4) * 2 机器和 <code class="notranslate">internlm-xcomposer-7b</code> 模型部署一个图文理解创作 Demo 。</p>
<h3>4.1 环境准备</h3>
<p>首先在 <a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a> 上选择 A100(1/4)*2 的配置。如下图所示：</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-8.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-8.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>接下来打开刚刚租用服务器的 <code class="notranslate">进入开发机</code>，并在终端输入 <code class="notranslate">bash</code> 命令，进入 <code class="notranslate">conda</code> 环境，接下来就是安装依赖。</p>
<p>进入 <code class="notranslate">conda</code> 环境之后，使用以下命令从本地克隆一个已有的<code class="notranslate">pytorch 2.0.1</code> 的环境</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">conda create --name xcomposer-demo --clone=/root/share/conda_envs/internlm-base</pre></div>
<p>然后使用以下命令激活环境</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">conda activate xcomposer-demo</pre></div>
<p>接下来运行以下命令，安装 <code class="notranslate">transformers</code>、<code class="notranslate">gradio</code> 等依赖包。请严格安装以下版本安装！</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install transformers==4.33.1 timm==0.4.12 sentencepiece==0.1.99 gradio==3.44.4 markdown2==2.4.10 xlsxwriter==3.1.2 einops accelerate</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/397f26d3-eba1-42d4-a241-8795ce5022ca"><img src="https://github.com/mattheliu/gitblog/assets/102272920/397f26d3-eba1-42d4-a241-8795ce5022ca" alt="Pasted image 20240107114327" style="max-width: 100%;"></a></p>
<h3>4.2 模型下载</h3>
<p><a href="https://studio.intern-ai.org.cn/" rel="nofollow">InternStudio</a>平台的 <code class="notranslate">share</code> 目录下已经为我们准备了全系列的 <code class="notranslate">InternLM</code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">mkdir -p /root/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-xcomposer-7b /root/model/Shanghai_AI_Laboratory</pre></div>
<blockquote>
<p>-r 选项表示递归地复制目录及其内容</p>
</blockquote>
<p>也可以安装 <code class="notranslate">modelscope</code>，下载模型的老朋友了</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install modelscope==1.9.5</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/20d9bcaf-9855-4ef9-85a1-a094966f3750"><img src="https://github.com/mattheliu/gitblog/assets/102272920/20d9bcaf-9855-4ef9-85a1-a094966f3750" alt="Pasted image 20240107114350" style="max-width: 100%;"></a><br>
在 <code class="notranslate">/root/model</code> 路径下新建 <code class="notranslate">download.py</code> 文件并在其中输入以下内容，并运行 <code class="notranslate">python /root/model/download.py</code> 执行下载</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">modelscope</span> <span class="pl-k">import</span> <span class="pl-s1">snapshot_download</span>, <span class="pl-v">AutoModel</span>, <span class="pl-v">AutoTokenizer</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-en">snapshot_download</span>(<span class="pl-s">'Shanghai_AI_Laboratory/internlm-xcomposer-7b'</span>, <span class="pl-s1">cache_dir</span><span class="pl-c1">=</span><span class="pl-s">'/root/model'</span>, <span class="pl-s1">revision</span><span class="pl-c1">=</span><span class="pl-s">'master'</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/0b865fa4-2235-48a2-a7c2-74a395733d43"><img src="https://github.com/mattheliu/gitblog/assets/102272920/0b865fa4-2235-48a2-a7c2-74a395733d43" alt="Pasted image 20240107114800" style="max-width: 100%;"></a></p>
<h3>4.3 代码准备</h3>
<p>在 <code class="notranslate">/root/code</code> <code class="notranslate">git clone InternLM-XComposer</code> 仓库的代码</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c1">cd</span> /root/code
git clone https://gitee.com/internlm/InternLM-XComposer.git
<span class="pl-c1">cd</span> /root/code/InternLM-XComposer
git checkout 3e8c79051a1356b9c388a6447867355c0634932d  <span class="pl-c"><span class="pl-c">#</span> 最好保证和教程的 commit 版本一致</span></pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/8aa51808-52f5-481d-944d-905497bf56c8"><img src="https://github.com/mattheliu/gitblog/assets/102272920/8aa51808-52f5-481d-944d-905497bf56c8" alt="Pasted image 20240107115512" style="max-width: 100%;"></a></p>
<h3>4.4 Demo 运行</h3>
<p>在终端运行以下代码：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate"><span class="pl-c1">cd</span> /root/code/InternLM-XComposer
python examples/web_demo.py  \
    --folder /root/model/Shanghai_AI_Laboratory/internlm-xcomposer-7b \
    --num_gpus 1 \
    --port 6006</pre></div>
<blockquote>
<p>这里 <code class="notranslate">num_gpus 1</code> 是因为InternStudio平台对于 <code class="notranslate">A100(1/4)*2</code> 识别仍为一张显卡。但如果有小伙伴课后使用两张 3090 来运行此 demo，仍需将 <code class="notranslate">num_gpus</code> 设置为 <code class="notranslate">2</code> 。</p>
</blockquote>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3"><strong>查看本教程5.2配置本地端口后</strong></a>，将端口映射到本地。在本地浏览器输入 <code class="notranslate">http://127.0.0.1:6006</code> 即可。我们以<code class="notranslate">又见敦煌</code>为提示词，体验图文创作的功能，如下图所示：</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/11264664-6b3f-4a7a-bade-569e779407cd"><img src="https://github.com/mattheliu/gitblog/assets/102272920/11264664-6b3f-4a7a-bade-569e779407cd" alt="Pasted image 20240107135211" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/132aedb0-a4e6-4592-9789-31ef8563404b"><img src="https://github.com/mattheliu/gitblog/assets/102272920/132aedb0-a4e6-4592-9789-31ef8563404b" alt="Pasted image 20240107135228" style="max-width: 100%;"></a></p>
<p>接下来，我们可以体验一下图片理解的能力，如下所示~</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/24979ac7-ecae-439d-953d-820bbbe4ea01"><img src="https://github.com/mattheliu/gitblog/assets/102272920/24979ac7-ecae-439d-953d-820bbbe4ea01" alt="Pasted image 20240107135339" style="max-width: 100%;"></a></p>
<h2>5. 通用环境配置</h2>
<h3>5.1 pip、conda 换源</h3>
<p>更多详细内容可移步至 <a href="https://help.mirrors.cernet.edu.cn/" rel="nofollow">MirrorZ Help</a> 查看。</p>
<h4>5.1.1 pip 换源</h4>
<p>临时使用镜像源安装，如下所示：<code class="notranslate">some-package</code> 为你需要安装的包名</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install -i https://mirrors.cernet.edu.cn/pypi/web/simple some-package</pre></div>
<p>设置pip默认镜像源，升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置，如下所示：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python -m pip install --upgrade pip
pip config <span class="pl-c1">set</span> global.index-url https://mirrors.cernet.edu.cn/pypi/web/simple</pre></div>
<p>如果您的 pip 默认源的网络连接较差，临时使用镜像源升级 pip：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">python -m pip install -i https://mirrors.cernet.edu.cn/pypi/web/simple --upgrade pip</pre></div>
<h4>5.1.2 conda 换源</h4>
<p>镜像站提供了 Anaconda 仓库与第三方源（conda-forge、msys2、pytorch 等），各系统都可以通过修改用户目录下的 <code class="notranslate">.condarc</code> 文件来使用镜像站。</p>
<p>不同系统下的 <code class="notranslate">.condarc</code> 目录如下：</p>
<ul>
<li><code class="notranslate">Linux</code>: <code class="notranslate">${HOME}/.condarc</code></li>
<li><code class="notranslate">macOS</code>: <code class="notranslate">${HOME}/.condarc</code></li>
<li><code class="notranslate">Windows</code>: <code class="notranslate">C:\Users\&lt;YourUserName&gt;\.condarc</code></li>
</ul>
<p>注意：</p>
<ul>
<li><code class="notranslate">Windows</code> 用户无法直接创建名为 <code class="notranslate">.condarc</code> 的文件，可先执行 <code class="notranslate">conda config --set show_channel_urls yes</code> 生成该文件之后再修改。</li>
</ul>
<p>快速配置</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">cat <span class="pl-s"><span class="pl-k">&lt;&lt;</span>'<span class="pl-k">EOF</span>' &gt; ~/.condarc</span>
<span class="pl-s">channels:</span>
<span class="pl-s">  - defaults</span>
<span class="pl-s">show_channel_urls: true</span>
<span class="pl-s">default_channels:</span>
<span class="pl-s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span>
<span class="pl-s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span>
<span class="pl-s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span>
<span class="pl-s">custom_channels:</span>
<span class="pl-s">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>
<span class="pl-s">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>
<span class="pl-s"><span class="pl-k">EOF</span></span></pre></div>
<h3>5.2 配置本地端口</h3>
<p>由于服务器通常只暴露了用于安全远程登录的 SSH（Secure Shell）端口，如果需要访问服务器上运行的其他服务（如 web 应用）的特定端口，需要一种特殊的设置。我们可以通过使用SSH隧道的方法，将服务器上的这些特定端口映射到本地计算机的端口。这样做的步骤如下：</p>
<p>首先我们需要配置一下本地的 <code class="notranslate">SSH Key</code> ，我们这里以 <code class="notranslate">Windows</code> 为例。</p>
<p>步骤①：在本地机器上打开 <code class="notranslate">Power Shell</code> 终端。在终端中，运行以下命令来生成 SSH 密钥对：（如下图所示）</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">ssh-keygen -t rsa</pre></div>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-13.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-13.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>步骤②： 您将被提示选择密钥文件的保存位置，默认情况下是在 <code class="notranslate">~/.ssh/</code> 目录中。按 <code class="notranslate">Enter</code> 键接受默认值或输入自定义路径。</p>
<p>步骤③：公钥默认存储在 <code class="notranslate">~/.ssh/id_rsa.pub</code>，可以通过系统自带的 <code class="notranslate">cat</code> 工具查看文件内容：（如下图所示）</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">cat <span class="pl-k">~</span><span class="pl-cce">\.</span>ssh<span class="pl-cce">\i</span>d_rsa.pub</pre></div>
<blockquote>
<p><code class="notranslate">~</code> 是用户主目录的简写，<code class="notranslate">.ssh</code> 是SSH配置文件的默认存储目录，<code class="notranslate">id_rsa.pub</code> 是 SSH 公钥文件的默认名称。所以，<code class="notranslate">cat ~\.ssh\id_rsa.pub</code> 的意思是查看用户主目录下的 <code class="notranslate">.ssh</code> 目录中的 <code class="notranslate">id_rsa.pub</code> 文件的内容。</p>
</blockquote>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-14.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-14.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>步骤④：将公钥复制到剪贴板中，然后回到 <code class="notranslate">InternStudio</code> 控制台，点击配置 SSH Key。如下图所示：</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-15.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-15.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>步骤⑤：将刚刚复制的公钥添加进入即可。</p>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-16.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-16.png" alt="Alt text" style="max-width: 100%;"></a></p>
<p>步骤⑥：在本地终端输入以下指令 <code class="notranslate">.6006</code> 是在服务器中打开的端口，而 <code class="notranslate">33090</code> 是根据开发机的端口进行更改。如下图所示：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 33090</pre></div>
<p><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/images/image-17.png"><img src="https://github.com/InternLM/tutorial/raw/main/helloworld/images/image-17.png" alt="Alt text" style="max-width: 100%;"></a></p>
<h3>5.3 模型下载</h3>
<h4><a href="https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#531-hugging-face"></a>5.3.1 Hugging Face</h4>
<p>使用 Hugging Face 官方提供的 <code class="notranslate">huggingface-cli</code> 命令行工具。安装依赖:</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install -U huggingface_hub</pre></div>
<p>然后新建 python 文件，填入以下代码，运行即可。</p>
<ul>
<li>resume-download：断点续下</li>
<li>local-dir：本地存储路径。（linux 环境下需要填写绝对路径）</li>
</ul>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">os</span>

<span class="pl-c"># 下载模型</span>
<span class="pl-s1">os</span>.<span class="pl-en">system</span>(<span class="pl-s">'huggingface-cli download --resume-download internlm/internlm-chat-7b --local-dir your_path'</span>)</pre></div>
<p>以下内容将展示使用 <code class="notranslate">huggingface_hub</code> 下载模型中的部分文件</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">os</span> 
<span class="pl-k">from</span> <span class="pl-s1">huggingface_hub</span> <span class="pl-k">import</span> <span class="pl-s1">hf_hub_download</span>  <span class="pl-c"># Load model directly </span>

<span class="pl-en">hf_hub_download</span>(<span class="pl-s1">repo_id</span><span class="pl-c1">=</span><span class="pl-s">"internlm/internlm-7b"</span>, <span class="pl-s1">filename</span><span class="pl-c1">=</span><span class="pl-s">"config.json"</span>)</pre></div>
<h4>5.3.2 ModelScope</h4>
<p>使用 <code class="notranslate">modelscope</code> 中的 <code class="notranslate">snapshot_download</code> 函数下载模型，第一个参数为模型名称，参数 <code class="notranslate">cache_dir</code> 为模型的下载路径。</p>
<p>注意：<code class="notranslate">cache_dir</code> 最好为绝对路径。</p>
<p>安装依赖：</p>
<div class="highlight highlight-source-shell"><pre class="notranslate">pip install modelscope==1.9.5
pip install transformers==4.35.2</pre></div>
<p>在当前目录下新建 python 文件，填入以下代码，运行即可。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">modelscope</span> <span class="pl-k">import</span> <span class="pl-s1">snapshot_download</span>, <span class="pl-v">AutoModel</span>, <span class="pl-v">AutoTokenizer</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-en">snapshot_download</span>(<span class="pl-s">'Shanghai_AI_Laboratory/internlm-chat-7b'</span>, <span class="pl-s1">cache_dir</span><span class="pl-c1">=</span><span class="pl-s">'your path'</span>, <span class="pl-s1">revision</span><span class="pl-c1">=</span><span class="pl-s">'master'</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/b6e224f9-9759-409c-b2bf-395e7d92106c"><img src="https://github.com/mattheliu/gitblog/assets/102272920/b6e224f9-9759-409c-b2bf-395e7d92106c" alt="Pasted image 20240107140559" style="max-width: 100%;"></a></p>
<h4>5.3.3 OpenXLab</h4>
<p>OpenXLab 可以通过指定模型仓库的地址，以及需要下载的文件的名称，文件所需下载的位置等，直接下载模型权重文件。</p>
<p>使用python脚本下载模型首先要安装依赖，安装代码如下：<code class="notranslate">pip install -U openxlab</code> 安装完成后使用 download 函数导入模型中心的模型。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">from</span> <span class="pl-s1">openxlab</span>.<span class="pl-s1">model</span> <span class="pl-k">import</span> <span class="pl-s1">download</span>
<span class="pl-en">download</span>(<span class="pl-s1">model_repo</span><span class="pl-c1">=</span><span class="pl-s">'OpenLMLab/InternLM-7b'</span>, <span class="pl-s1">model_name</span><span class="pl-c1">=</span><span class="pl-s">'InternLM-7b'</span>, <span class="pl-s1">output</span><span class="pl-c1">=</span><span class="pl-s">'your local path'</span>)</pre></div>
<h2>6. 课后作业</h2>
<p>提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。</p>
<p><strong>基础作业：</strong></p>
<ul>
<li>
<p>使用 InternLM-Chat-7B 模型生成 300 字的小故事（需截图）。<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/7fe3de84-8c0f-4a87-a772-a3e38d272d9d"><img src="https://github.com/mattheliu/gitblog/assets/102272920/7fe3de84-8c0f-4a87-a772-a3e38d272d9d" alt="Pasted image 20240107105513" style="max-width: 100%;"></a></p>
</li>
<li>
<p>熟悉 hugging face 下载功能，使用 <code class="notranslate">huggingface_hub</code> python 包，下载 <code class="notranslate">InternLM-20B</code> 的 config.json 文件到本地（需截图下载过程）。<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/dbab411b-d777-4401-a740-dd79d2eacdeb"><img src="https://github.com/mattheliu/gitblog/assets/102272920/dbab411b-d777-4401-a740-dd79d2eacdeb" alt="Pasted image 20240107142851" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/8382d786-3f45-4752-98dc-a465829f6420"><img src="https://github.com/mattheliu/gitblog/assets/102272920/8382d786-3f45-4752-98dc-a465829f6420" alt="Pasted image 20240107142923" style="max-width: 100%;"></a><br>
<strong>进阶作业（可选做）</strong></p>
</li>
<li>
<p>完成浦语·灵笔的图文理解及创作部署（需截图）</p>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/f8952b85-9bb1-4600-9ef9-af77499aee46"><img src="https://github.com/mattheliu/gitblog/assets/102272920/f8952b85-9bb1-4600-9ef9-af77499aee46" alt="Pasted image 20240107135211" style="max-width: 100%;"></a><br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/57e14dc0-04f4-4bf5-b8e2-0556146ea408"><img src="https://github.com/mattheliu/gitblog/assets/102272920/57e14dc0-04f4-4bf5-b8e2-0556146ea408" alt="Pasted image 20240107135228" style="max-width: 100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/3043f928-7561-4267-812b-33a36fa73f97"><img src="https://github.com/mattheliu/gitblog/assets/102272920/3043f928-7561-4267-812b-33a36fa73f97" alt="Pasted image 20240107135339" style="max-width: 100%;"></a></p>
<ul>
<li>完成 Lagent 工具调用 Demo 创作部署（需截图）<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mattheliu/gitblog/assets/102272920/70e9584a-77c5-4959-805f-866993bca1fd"><img src="https://github.com/mattheliu/gitblog/assets/102272920/70e9584a-77c5-4959-805f-866993bca1fd" alt="Pasted image 20240107113802" style="max-width: 100%;"></a></li>
</ul>
<p><strong>整体实训营项目：</strong></p>
<p>时间周期：即日起致课程结束</p>
<p>即日开始可以在班级群中随机组队完成一个大作业项目，一些可提供的选题如下：</p>
<ul>
<li>人情世故大模型：一个帮助用户撰写新年祝福文案的人情事故大模型</li>
<li>中小学数学大模型：一个拥有一定数学解题能力的大模型</li>
<li>心理大模型：一个治愈的心理大模型</li>
<li>工具调用类项目：结合 Lagent 构建数据集训练 InternLM 模型，支持对 MMYOLO 等工具的调用</li>
</ul>
<p>其他基于书生·浦语工具链的小项目都在范围内，欢迎大家充分发挥想象力。</p>
</div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://mattheliu.github.io">Leonliuzx-Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","mattheliu/mattheliu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
