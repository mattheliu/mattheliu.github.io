<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Leonliuzx-Blog</title><link>https://mattheliu.github.io</link><description>Leonliuzx-Blog</description><copyright>Leonliuzx-Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://mattheliu.github.io</link></image><lastBuildDate>Wed, 24 Apr 2024 07:21:02 +0000</lastBuildDate><managingEditor>Leonliuzx-Blog</managingEditor><ttl>60</ttl><webMaster>Leonliuzx-Blog</webMaster><item><title>书生·浦语大模型第二期实战营作业速通</title><link>https://mattheliu.github.io/post/shu-sheng-%C2%B7-pu-yu-da-mo-xing-di-er-qi-shi-zhan-ying-zuo-ye-su-tong.html</link><description># 轻松玩转书生·浦语大模型趣味 Demo&#13;
Homework - Demo&#13;
提交方式：在 CSDN 、知乎 、Github 等平台上传作业后，将链接贴至 [飞书作业板](https://aicarrier.feishu.cn/wiki/TqjawZsoqiiRXEkRpcScmKAAn8d?table=tblNCUy9PeGmgd9I&amp;view=vewQagjCL1) 即可。</description><guid isPermaLink="true">https://mattheliu.github.io/post/shu-sheng-%C2%B7-pu-yu-da-mo-xing-di-er-qi-shi-zhan-ying-zuo-ye-su-tong.html</guid><pubDate>Wed, 24 Apr 2024 07:19:32 +0000</pubDate></item><item><title>第一期书生浦语大模型实战营-轻松玩转书生·浦语大模型趣味 Demo</title><link>https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---qing-song-wan-zhuan-shu-sheng-%C2%B7-pu-yu-da-mo-xing-qu-wei-%20Demo.html</link><description>## 1 大模型及 InternLM 模型简介&#13;
&#13;
### 1.1 什么是大模型？&#13;
&#13;
  大模型通常指的是机器学习或人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。</description><guid isPermaLink="true">https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---qing-song-wan-zhuan-shu-sheng-%C2%B7-pu-yu-da-mo-xing-qu-wei-%20Demo.html</guid><pubDate>Wed, 24 Apr 2024 07:06:43 +0000</pubDate></item><item><title>第一期书生浦语大模型实战营-书生·浦语大模型全链路开源体系​</title><link>https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---shu-sheng-%C2%B7-pu-yu-da-mo-xing-quan-lian-lu-kai-yuan-ti-xi-%E2%80%8B.html</link><description>![Pasted image 20240104121846](https://github.com/mattheliu/gitblog/assets/102272920/dc64b5dd-6607-416b-8ad7-7a89ccd10bd2)&#13;
&#13;
### 介绍&#13;
- 书生·浦语大模型实战营旨在提供大模型开发和应用技能的学习，帮助学员掌握相关技能，从而获得丰富经验。</description><guid isPermaLink="true">https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---shu-sheng-%C2%B7-pu-yu-da-mo-xing-quan-lian-lu-kai-yuan-ti-xi-%E2%80%8B.html</guid><pubDate>Sun, 25 Feb 2024 07:13:17 +0000</pubDate></item><item><title>第一期书生浦语大模型实战营-OpenCompass 大模型评测</title><link>https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--OpenCompass%20-da-mo-xing-ping-ce.html</link><description># 大模型评测教程&#13;
&#13;
随着人工智能技术的快速发展， 大规模预训练自然语言模型成为了研究热点和关注焦点。</description><guid isPermaLink="true">https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--OpenCompass%20-da-mo-xing-ping-ce.html</guid><pubDate>Sun, 25 Feb 2024 07:13:17 +0000</pubDate></item><item><title>第一期书生浦语大模型实战营-LMDeploy 大模型量化部署实践</title><link>https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--LMDeploy%20-da-mo-xing-liang-hua-bu-shu-shi-jian.html</link><description>![](cover.jpg)&#13;
&#13;
# LMDeploy 的量化和部署&#13;
&#13;
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;&#13;
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;&#13;
&#13;
- [1 环境配置](#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)&#13;
- [2 服务部署](#2-%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2)&#13;
  - [2.1 模型转换](#21-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2)&#13;
    - [2.1.1 在线转换](#211-%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2)&#13;
    - [2.1.2 离线转换](#212-%E7%A6%BB%E7%BA%BF%E8%BD%AC%E6%8D%A2)&#13;
  - [2.2  TurboMind 推理+命令行本地对话](#22--turbomind-%E6%8E%A8%E7%90%86%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D)&#13;
  - [2.3 TurboMind推理+API服务](#23-turbomind%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)&#13;
  - [2.4 网页 Demo 演示](#24-%E7%BD%91%E9%A1%B5-demo-%E6%BC%94%E7%A4%BA)&#13;
    - [2.4.1 TurboMind 服务作为后端](#241-turbomind-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)&#13;
    - [2.4.2 TurboMind 推理作为后端](#242-turbomind-%E6%8E%A8%E7%90%86%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)&#13;
  - [2.5 TurboMind 推理 + Python 代码集成](#25-turbomind-%E6%8E%A8%E7%90%86--python-%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90)&#13;
  - [2.6 这么多，头秃，有没有最佳实践](#26-%E8%BF%99%E4%B9%88%E5%A4%9A%E5%A4%B4%E7%A7%83%E6%9C%89%E6%B2%A1%E6%9C%89%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)&#13;
    - [2.6.1 方案实践](#261-%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5)&#13;
    - [2.6.2 模型配置实践](#262-%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E5%AE%9E%E8%B7%B5)&#13;
- [3 模型量化](#3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96)&#13;
  - [3.1 KV Cache 量化](#31-kv-cache-%E9%87%8F%E5%8C%96)&#13;
    - [3.1.1 量化步骤](#311-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)&#13;
    - [3.1.2 量化效果](#312-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)&#13;
  - [3.2 W4A16 量化](#32-w4a16-%E9%87%8F%E5%8C%96)&#13;
    - [3.2.1 量化步骤](#321-%E9%87%8F%E5%8C%96%E6%AD%A5%E9%AA%A4)&#13;
    - [3.2.2 量化效果](#322-%E9%87%8F%E5%8C%96%E6%95%88%E6%9E%9C)&#13;
  - [3.3 最佳实践](#33-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5)&#13;
- [参考资料](#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)&#13;
- [附录1：TritonServer 作为推理引擎](#%E9%99%84%E5%BD%951tritonserver-%E4%BD%9C%E4%B8%BA%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E)&#13;
  - [TritonServer环境配置](#tritonserver%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE)&#13;
  - [TritonServer推理+API服务](#tritonserver%E6%8E%A8%E7%90%86api%E6%9C%8D%E5%8A%A1)&#13;
  - [TritonServer 服务作为后端](#tritonserver-%E6%9C%8D%E5%8A%A1%E4%BD%9C%E4%B8%BA%E5%90%8E%E7%AB%AF)&#13;
&#13;
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;&#13;
&#13;
&#13;
## 1 环境配置&#13;
&#13;
首先我们可以使用 `vgpu-smi ` 查看显卡资源使用情况。</description><guid isPermaLink="true">https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying--LMDeploy%20-da-mo-xing-liang-hua-bu-shu-shi-jian.html</guid><pubDate>Sun, 25 Feb 2024 07:13:17 +0000</pubDate></item><item><title>第一期书生浦语大模型实战营-基于 InternLM 和 LangChain 搭建你的知识库</title><link>https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---ji-yu-%20InternLM%20-he-%20LangChain%20-da-jian-ni-de-zhi-shi-ku.html</link><description>## 1 环境配置&#13;
&#13;
### [](https://github.com/InternLM/tutorial/blob/main/langchain/readme.md#11-internlm-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2)1.1 InternLM 模型部署&#13;
&#13;
在 [InternStudio](https://studio.intern-ai.org.cn/) 平台中选择 A100(1/4) 的配置，如下图所示镜像选择 `Cuda11.7-conda`，如下图所示：&#13;
&#13;
[![Alt text](https://github.com/InternLM/tutorial/raw/main/langchain/figures/image.png)](https://github.com/InternLM/tutorial/blob/main/langchain/figures/image.png)&#13;
&#13;
接下来打开刚刚租用服务器的 `进入开发机`，并且打开其中的终端开始环境配置、模型下载和运行 `demo`。</description><guid isPermaLink="true">https://mattheliu.github.io/post/di-yi-qi-shu-sheng-pu-yu-da-mo-xing-shi-zhan-ying---ji-yu-%20InternLM%20-he-%20LangChain%20-da-jian-ni-de-zhi-shi-ku.html</guid><pubDate>Sun, 25 Feb 2024 07:13:17 +0000</pubDate></item></channel></rss>